# 2025-03-19

## 1

`模型时代` 黄仁勋在GTC2025上的一个重点观点：AI系统要先纵向扩展(Scale Up)，再横向扩展(Scale Out)

在2025年GTC大会上，NVIDIA CEO黄仁勋在演讲中花了一个比较大篇幅谈了了一个观点："在（AI系统）横向扩展之前，必须先纵向扩展。他随后解释，"因为随着AI模型越来越复杂，特别是随着推理模型(Reasoning Models)的兴起，我们需要的计算量呈爆炸性增长，而一个简单的事实是：传统的计算扩展方法已经无法满足需求。

我觉得这个观点比较重要，所以把这部分单拿出来，做一个浅层总结。

一、什么是纵向扩展与横向扩展？

为了让概念更加容易理解，我们可以用一个简单的比喻：横向扩展(Scale Out)就像是建造一个由许多小型工作室组成的园区。每个工作室有独立的设备和团队，它们通过电话或邮件相互协调。当你需要更多产能时，就建造更多工作室。

纵向扩展(Scale Up)则像是打造一个巨大的开放式工作间。所有人在同一空间工作，共享高级设备，能够面对面即时沟通。当你需要更多产能时，你把这个空间做得更大、更高效。

显然，这两种方法都能创造更大规模的产出，但是横向扩展的弹性更大，但是系统的沟通开销比较高；纵向扩展的效率更高，但是扩展空间有限。

实际上，在整个计算历史上，扩展计算能力的方法就一直在纵向扩展(Scale Up)和横向扩展(Scale Out)之间摇摆。

二、问题的起点

在20世纪60-80年代，计算世界由IBM、DEC等公司的大型机和小型机主导。这是纵向扩展的黄金时代：要获得更强大的计算能力，解决方案是构建更大、更复杂的单一系统。IBM的System/360和后来的大型机系列代表了这一理念的巅峰——单一系统不断增强，价格昂贵但性能强大。

90年代，个人计算机的普及和互联网的兴起带来了转变。谷歌、亚马逊等新贵发现，通过横向扩展——连接成千上万台普通服务器——可以比昂贵的大型机更经济地处理海量数据。谷歌的GFS(Google File System)和MapReduce、开源的Hadoop生态系统，都基于同一理念：使用大量普通机器组成集群，通过软件处理硬件故障。

云计算时代更是将横向扩展推向极致。亚马逊AWS的成功建立在"牺牲单机可靠性，通过软件保证整体可靠性"的理念上。"面向故障的系统设计"成为云架构的口号，意味着单个节点（坏掉）不那么重要，系统可以无限扩展。

也正是在这个时代，国内出现了去IOE的概念，者也是横向扩展，对纵向扩展打出的一拳重击。

然而，机器学习特别是深度学习的兴起，又开始挑战纯横向扩展模型的局限。

2012年，AlexNet在ImageNet竞赛的胜利标志着GPU计算的崛起，而GPU天生就是一种纵向扩展的架构。随着模型规模从百万参数增长到数千亿参数，数据在不同节点间的移动成为瓶颈——在分离的节点间移动海量数据既耗能又增加延迟。（回想一下我们的比喻，不同工作室在园区里通过打电话、会面交流工作，总是不如同一个团队在一个办公室大平层里交流更方便）

三、所以，英伟达为什么要打造纵向的超级节点？

黄仁勋的理念代表了计算历史上的一次新综合：先通过极致的纵向扩展创造"超级节点"，再通过高效的光互连实现横向扩展，最终构建能够处理新一代AI工作负载的数据中心。

这不是简单地回到大型机时代，而是基于深刻理解AI工作负载特性后的新平衡。

黄仁勋认为，在AI计算中，必须先将单个系统做到极致（纵向扩展），然后才考虑增加系统数量（横向扩展）。这不是随意的选择，而是基于AI工作负载特性的必然结果。

四、因为推理工作很特殊

为了理解黄仁勋的论点，我们首先需要认识到AI推理工作的特殊性。黄仁勋详细解释了AI推理涉及的两个关键阶段：预填充(Prefill)阶段和解码(Decode)阶段。

在预填充阶段，系统处理输入信息，建立对问题的理解。就像一个人阅读一篇长文章，在回答问题前先理解内容。而在解码阶段，系统生成回应，一个词一个词地输出。

黄仁勋在演讲中用一个婚礼排座位的例子说明这一过程的复杂性："对于像DeepSeek-R1这样的推理模型，为了生成一个有意义的回答，它可能需要思考8000多个词(tokens)，而普通模型只需要约400个。这意味着token增加了20倍。”，而“计算量提高了150倍”。

每生成一个词，系统都需要加载整个模型——可能有数千亿甚至数万亿参数。黄仁勋描述道："每生成一个词，系统都要加载数万亿字节的信息。加载整个模型，产生一个词；再加载整个模型，产生下一个词，如此反复数千次。"

如果这些数据需要在物理分离的系统间传输，效率就会急剧下降。这就是为什么纵向扩展——让数据留在一个紧密集成的系统内——如此重要的原因。

五、所以，英伟达重新思考架构

黄仁勋和NVIDIA团队因此提出了一个根本性的架构重构。

他首先展示了传统的HGX系统——一个重达70磅的硬件模块，包含8个通过"NVLink"连接的GPU。

"这是我们最初尝试纵向扩展的方式，"他说，”但是遇到了瓶颈。"

现在突破来自于一个关键决策：将NVLink交换机从GPU主板中分离出来。"我们意识到，要实现极致的纵向扩展，必须重新思考NVLink的工作方式。"黄仁勋解释，"我们做了一个看似简单但实际上非常关键的改变：将NVLink交换机从主板中分离。"

这一变革使NVIDIA能够创建Grace Blackwell NVLink72系统——一个集成了72个GPU和36个Grace CPU的液冷机架。通过这种架构重构，NVIDIA实现了黄仁勋所说的"有史以来最极致的纵向扩展"。

这个NVLink72系统表现得像一个单一的巨型GPU：所有72个GPU可以无缝协作，数据可以在所有GPU间高速移动，无需经过慢速网络，系统可以动态调整资源分配，根据工作负载的变化。"这就像是把72个音乐家放在一个舞台上同时演奏，而不是在72个不同的房间里通过视频会议协调。"黄仁勋比喻道。

六、具体表现：从数字看变革

黄仁勋用一组数字说明这一架构变革的意义。在ISO功率条件下（相同电力输入），Blackwell比Hopper提供了25倍的性能。在推理模型上，Blackwell比Hopper快40倍。与Hopper相比，基于Blackwell的AI工厂性能提高68倍，而未来基于Rubin的性能将提高900倍。

"这不是简单的增量提升，"黄仁勋强调，"这是计算范式的根本转变。"

他还特别提到了一个有趣的细节："当Blackwell开始大规模出货时，不要把Hopper送人。"这句话引发了全场笑声，但背后是一个严肃的观点：新架构的性能提升如此巨大，以至于上一代产品几乎立刻变得过时。

七、电力经济学：真正的限制因素

在演讲中，黄仁勋反复提到一个现实：电力是数据中心扩展的终极限制。"每个数据中心最终都会受到电力限制，"他解释，"你的收入直接与可用电力挂钩。"

这就是为什么能源效率如此重要的原因。通过极致的纵向扩展，NVIDIA能够减少数据传输的能源消耗，提高计算单元的利用率，最大化每瓦电力能够处理的工作量。

"当我们说Blackwell比Hopper提供25倍性能时，我们是在相同电力输入的条件下比较，"黄仁勋解释，"这意味着每瓦电力能够完成25倍的工作，直接转化为25倍的收入潜力。"

他用一个具体例子说明：一个100兆瓦的AI工厂配备Hopper能产生约3亿tokens/秒，而同样功率配备Blackwell则能产生40倍的tokens，意味着40倍的revenue tokens。按照每百万tokens 10美元计算，这将带来巨大的收入差异。

八、从纵向到横向：完整的扩展战略

尽管黄仁勋强调先纵向扩展的重要性，他也明确指出，最终的AI工厂需要横向扩展到数百万GPU。问题是：如何高效地连接这些纵向扩展的系统？

"在纵向扩展中，我们使用铜线，因为它有最高的可靠性和最低的成本，"黄仁勋解释，"但铜线的有效距离只有一两米。"当数据中心扩大到"体育场大小"时，需要光纤通信。

然而，传统的光纤解决方案效率低下："传统的光模块每个消耗30瓦电力，成本约1000美元。对于一个拥有100万个GPU的系统，每个GPU需要6个光模块，这将消耗180兆瓦电力仅用于信号转换！"

为了解决这一挑战，NVIDIA开发了革命性的硅光子技术，将光器件直接集成到交换机中。这项技术减少了4倍的激光器数量，能源效率提高3.5倍，信号完整性提高63倍，大规模组网可靠性提高10倍。

"在一个数据中心，这可以节省数十兆瓦的电力，"黄仁勋计算道，"60兆瓦相当于100个Rubin Ultra机架的电力，这些电力现在可以用于实际计算而不是数据传输。"

九、未来展望：从架构到基础设施

黄仁勋提出了NVIDIA的长期路线图，包括从当前的Blackwell到未来的Blackwell Ultra、Vera Rubin、Rubin Ultra，最终到2028年的Feynman架构。

"我们制定这样长期的路线图，是因为AI基础设施建设需要提前2-3年规划，"黄仁勋解释，"这不像买笔记本电脑，而是需要提前规划土地、电力和资本支出。"

他特别介绍了Vera Rubin NVLink144和Rubin Ultra NVLink576架构的细节。Vera Rubin由Rubin GPU和Vera CPU组成，Vera CPU拥有88个定制Arm核心、176个线程。Rubin Ultra由4块掩模尺寸的GPU组成，拥有1TB HBM4e内存，FP4峰值推理能力可达100PFLOPS。

Rubin Ultra NVL576的FP4峰值推理算力高达15EFLOPS，是GB300 NVL72的14倍。黄仁勋用一张对比图形象地展示了从Blackwell到Rubin的尺寸增长，让观众直观地感受到了技术进步的速度。

十、重新定义计算思维

黄仁勋的"先纵向扩展，再横向扩展"理念代表了对传统计算架构的根本重新思考。它源于对AI工作负载特性的深刻理解，并针对能源效率、计算密度和数据移动成本进行了全面优化。

正如黄仁勋在演讲结束时所说："传统的分布式计算模型已经不足以应对AI推理的挑战。我们需要新的思维、新的架构和新的基础设施，而这正是我们正在建造的——面向AI时代的下一代计算基础设施。"

他概括道："我们的路线图是每年一次更新，架构每两年一次更新，每年一条新产品线，性能倍数不断提升。我们在硅片、网络或系统机箱方面分批承担风险，以推动行业前进，同时追求这些令人难以置信的技术。"

通过这种方法，NVIDIA正在构建能够支持从推理AI到物理AI和机器人技术各种工作负载的新一代AI工厂，为未来的计算奠定基础。而这一切的核心理念，就是黄仁勋反复强调的那句话："在横向扩展之前，必须先纵向扩展。"

## 2

最近了解到一些情况，去年算力中心计算租金，比如H100的租金，是按照1.5年回本算的，由于训练需求大大降低，年初已经降低到3年回本算租金，这几天我已经接触到6年回本算租金的了，租金下跌，可以说惨不忍睹。。

前期一窝蜂上算力，现在很多大模型不再训练的，投入收缩，算力成立第一批受难者了。。惨啊。

B端业务永远不好做，只有C端才是永恒的[泪][泪]

## 3

我觉得有一个值得关注的动向，那就是美国尤其是特朗普政府对中国海运业、造船业快速发展的阻击。特朗普或许正在回归控制、垄断、占有全球关键性物质要素资源的“原始底层逻辑”。

--2 月 21 日，特朗普上台一个月，美国贸易代表办公室（USTR）宣布，拟对中国船只增收“港口费”：对中国海运公司拥有的船只，每艘征收最高 100 万美元的单次港口准入费，或是对进入美国港口的每艘船舶的净吨位货物收取 1000 美元的单次费用；对中国建造的船舶，每次停靠美国港口需缴纳高达 150 万美元的费用；对船队中哪怕只有一艘中国建造的船舶或在中国船厂订购的船舶，船舶运营商也需缴纳至少50万美元的费用。USTR就上述拟采取的措施向公众征求意见。

--特朗普政府此举有一个“前情”：去年4月，还是拜登在任时，美国就府针对中国在海事、物流和造船业发起“301 调查”。今年1月16日，拜登下台前4天，他任下的USTR发布调查结果，声称中国在海事、物流和造船业领域的地位“对美国商业造成负担或限制”。可以说，这一调查结果为特朗普上台USTR的后续动作铺平了道路。

--在美国国会，五花八门的针对中国海运业的提案被提出，除了“港口费”，还有对与中国有关的船舶征收更高的“吨位税”“灯塔费”等等。值得注意的是，上面说过的USTR希望对中国船只征收的“港口费”，获得了来自两党议员的支持，也得到了许多工会组织的支持。

2月11日，美国众议院国土安全委员会运输和海事安全小组委员会开了一场听证会，主题就是“研究中国在西半球的战略港口投资及其对国土安全的影响”，讨论中国对美国港口等基础设施的所谓“安全威胁”。一些议员公开叫嚣，“美国不接受中国控制对美国本土安全、军事准备、经济稳定至关重要的基础设施的局面”，呼吁“西半球国家对中国‘去风险’”。

--与此同时，美国内部也在推动“振兴美国港口”的议程，比如加强对港口基础设施的投资，激励美国国内制造更多集装箱起重机，把货物处理设备进口“友岸外包”等等。

--在舆论上，美国也在对中国的海运业进行“捧杀”，塑造“中国威胁”“中国控制世界航运”的形象：USTR的调查称，中国控制着全球 95％的航运集装箱生产和 86％的多式联运底盘车供应；全球交付的船舶中，有一半以上是由中国船厂建造等等。

--如果“港口费”等措施落地，当然会对中国的海运业、造船业乃至出口造成冲击，但也会对美国的进口商、出口商、承运人等造成冲击。此外，全球航运的效率和成本也会受到负面影响，因为太多的船队中都有中国建造的船只。

世界航运理事会总裁兼首席执行官乔·克拉梅克认为，如果实施“港口费”，将对美国供应链所有部门造成广泛的经济损失，“这些费用将导致停靠美国港口的次数减少、美国消费者价格上涨，并对出口商、特别是美国农民造成严重冲击”。也有美国学者认为，一些航运公司未来可能将尝试把船停靠到墨西哥和加拿大港口，然后把货再通过卡车运到美国。

不过，“港口费”究竟能否落地，怎样实施，都还有很大不确定性。

--无论“港口费”是否落地，美国在这一领域的长期动向和意图都已更加清晰：《金融时报》一篇文章认为，特朗普对全球贸易的广泛改革，并不仅限于商品或关税。美国正试图重塑全球贸易力量平衡，尽管这能否成功远未可知。

政治盟友与国家对手：特朗普为何喜欢普京？

Donald Trump seeks a grand bargain with Vladimir Putin

很多人觉得非常奇怪，他们对特朗普为何如此喜爱普京感到难以理解，这种莫名的好感其实是当今国际局势中非常重要的一部分：

即，特朗普为何对普京怀有好感？

事实上，尽管很多人宣称特朗普被普京收买了，并毫无证据地断定前者是后者安插在美国的高级间谍，但实话实说，特朗普并非俄国间谍，他对普京的好感与其说是不可告人的交易，不如说是单纯基于个人的政治利益和价值偏好。

要明白这一点，就需要理解，在特朗普眼里，谁是他的主要敌人？

是俄国么？是中国么？

显然不是。

特朗普的主要敌人，乃是美国民主党和一部分坚决反对特朗普的共和党建制派，这些政治力量，乃是2016年和2024年抵抗特朗普崛起的主谋，更是2021年之后试图以各种司法和经济手段打击特朗普甚至“迫害”他家族的政治死敌。

所以，从特朗普自身来说，2025年及以后他的所有政治目标，都在于尽可能削弱其政治对手的资源和地位，强化自身的政治基础和力量，从而在日益残酷的美国国内政治斗争中开拓更大的生存空间。

在这个过程中，谁是特朗普的盟友？谁是特朗普的敌人？

这个是非常明确的。

在2016年的总统大选中，普京控制下的俄罗斯媒体和互联网博主，在英文互联网宣传战中是特朗普最坚定的支持者，俄罗斯大外宣《今日俄罗斯》在2016年大选中是极少数支持特朗普的正规媒体，大量打击特朗普对手的宣称素材通过俄国媒体和自媒体散播出来，成为打击民主党和希拉里竞选的有力武器。

在2024年大选中，俄罗斯媒体和自媒体同样与特朗普政治集团在选举宣传战中并肩作战，譬如俄罗斯自媒体和正规媒体大量转发亲特朗普的自媒体博主讯息（如Joe Rogan、Tucker Carlson）以攻击民主党和共和党建制派，而亲特朗普的自媒体博主则支持了俄国在乌克兰的战争立场。

而且，在西方文化战争中，普京乃是特朗普的亲密盟友，他们都反对白左，对极端女权深恶痛绝，对畸形的LGBT不屑一顾，对奥巴马、默克尔主导的非基督教式多元化政策极为反感。在这种日益激化的文化辩论战争中，特朗普和普京乃是亲密的盟友而非敌人，而这种文化战争对特朗普本人的政治利益来说极为重要。

因此，从特朗普个人政治利益来说，普京乃是他不折不扣的盟友和同志：

在两次决定特朗普命运的选举战争中，他们共用一个战壕；

在对抗白左的文化战争中，他们呼吸与共；

某种程度上说，特朗普就像1991年八一九政变中的叶利钦，普京就像当时的西方——在八一九政变中，自诩自由主义者的叶利钦面对苏联保守派的抓捕时，他能够依赖的力量，除了本派系的街头群众，还有西方自由主义意识形态世界提供的重大外交、经济和战略支持——西方影响力越大，叶利钦在苏联国内的政治地位就越稳固。尽管从国家利益来说，叶利钦与西方实际上是存在重大利害冲突的。

特朗普与普京就是如此，在个人政治利益上，特朗普与普京是久经考验的盟友；但从国家利益来看，双方又存在着难以回避的矛盾。

这种关系导致了特朗普对俄政策的复杂性，就像1991年叶利钦面对西方时所做的那样，叶利钦愿意对西方做出巨大的让步，这并不是仅仅对西方怀有感激，而是西方影响力的扩大符合叶利钦自身的政治利益，而西方与叶利钦共同的价值观和战斗经历，让后者更愿意相信西方而非苏联建制派。

这就是特朗普与普京关系最复杂的地方，他愿意对普京做出相当大的让步，不仅仅是基于国家战略的重大举措，更是个人政治利益的考量。



## 4

有些案子就是把司法系统架起来了，导致判也不是，不判也不是。大家在热搜上看见的`爱犬被毒死主人追凶900天`这个案子就是个典型。几家媒体基本上是每隔100天推一次。你看上去一个很简单的毒狗案，为什么反复拖延那么久，就是不去判呢？很多媒体会和你讲之前的案例，你还能搜到大量的专家科普，都说这是有罪。那么为什么不处理呢？

我要是告诉你，之前几个案子判决逻辑是有问题的，同时那些专家的科普是故意造假，把无罪说有罪，你能接受吗？

这类案子，整体都很简单。就是有人讨厌狗，所以投药，把狗药死了。最后要追责。司法上核心问题表面上是投的药是否对人体有害。比如这个案子里面根据报导说投放的药物是对人体有害的，所以就立案了。

相关有些案件判的很重。有些地方法律意识不是很强。所以会出现公安机关开始的时候用毁坏财务罪把人抓了，然后检察院再套投放危险物质罪，去重判3年以上的案例。这些案例无一列外都是有问题的。他们采用的逻辑是犯罪分子投放的含有鼠药的猪肝，可能通过沾染到小动物身上进入家庭毒害到人，所以是危害公众安全。

可是这个事情在科学上其实是不可能的。因为沾染并且危害到人的计量要求是比较高的。简单来说就是法院用了一个把“不能犯”当有罪的案例。

结果判了也就判了，当时是以为宣传可以“震慑”住大众，不做类似行为。

但是现实是反效果。我国受过高等教育的人太多，很难吓唬住他们。后期比较常见的，是投放对人体无害，但是对宠物猫狗有害的日常物质。其中比较常见的是异烟肼以及某类兽药。这类药物对人体无害，所以套不进这些罪名。同时猫狗尸检甚至都没办法确定死因，就没办法搞出证据链条来限制相关行为。

各地更是多发类似事件。比如大连甚至出现过有人投放类似药物，一次性毒杀上百条狗的事情。基层基本属于束手无策。

那么你如果上网去搜索，却会发现很多专家和法学学者，说这是个有罪行为。比如一些知名媒体采访的专家就说投放异烟肼是犯罪，因为投放的是“三类致癌物”。大家一听就害怕。可是三类致癌物是个什么概念呢？大家常喝的咖啡里的咖啡因，就是典型的三类致癌物。

这种东西，对读过书的年轻一代，就没什么限制作用。唬不住他们。

那么你说在这样的一个大背景下面，还要不要根据之前的方式，去判决呢？更何况真正造成严重猫狗损害的案例，现阶段基本就没有办法限制呢？

像我的思路相对简单明了。依法没办法管的事情，也确实想不出办法，那也就只能认。呼吁大家给狗出门带嘴套，防止狗狗误食有害物质，对狗好，对人也好。坦荡点把问题讲清楚，就没有那么多问题了。

## 5

在各个地方调研时，基层干部对12345热线的反感溢于言表。对此，基层干部们给出的主要原因有二：

一是“可解决的不用打，解决不了的打了也没用”，12345热线表现出高能耗、低效能的运转困境。

二是“市民热线成了刁民热线，干部陷入大量琐碎且无理的问题中”。

这两个理由都值得进一步分析和思考，背后的逻辑有共同之处，也有差异之处。

一是权责不匹配下的“体制空转”问题。

村民拨打12345反映诉求后，由市级层面的12345热线平台将诉求内容来分类和定位，确定负责处理该诉求的责任单位。然而，这样一种以事分流的派单机制受信息密度的限制，派单员难以通过脱离治理情境的一个电话就能清楚地了解市民的诉求内容并将其分类，而且越是复杂难处理的问题，就越是没办法在前端完成甄别与分类。因此，大量无法顺利分类的复杂工单就以属地原则的名头，被下派给乡街村社。

与此同时，充分了解信息、确认与定位问题、处理复杂工单、安抚群众等一系列成本和压力也转移给属地，形成了权责之间的巨大错位。

一方面，无法顺利甄别与分流的复杂工单、困难工单只能依靠属地原则进行兜底，大量不好处理解决的剩余工单涌入基层属地，使得属地工单具有极为明显的问题化特征。

另一方面，基层属地在职责同构、压力型体制等结构性因素的影响下，天然具有权小责大的特点，缺乏相应的治理资源来应对这些复杂问题。

大量复杂工单以属地原则涌入基层，基层解决问题的能力又极其有限，在考核压力下，基层工作人员只能将大量精力花费在完成规定性工作上，以求“尽职免责”：全面了解工单内容、寻找可行的解决办法、完成各个环节的工作与留痕。

最终带来“体制空转”的问题：基层干部在无能力实质化解问题的情况下，仍需要投入大量时间精力去做好各个环节的规定性动作，以做到“程序完整”来消除工单未解决带来的考核影响。于是12345热线陷入了干部很忙但问题没解决，干部无意义而群众也不满意的怪圈之中。

因此，“体制空转”的核心逻辑在于：12345热线跨层级与去情境的特征，使其分流治理的效果具有内生缺陷，大量问题工单无法有效地在前端甄别与分流，只能以属地兜底的方式来填补缝隙。

与此同时，12345热线重塑了属地政府的回应责任，形成了回应率、时效性、解决率、满意率等指标考核，但并没有增加属地政府的回应能力。在问题工单、考核压力、资源约束所形成的挤压型结构下，基层干部只能通过安抚摆平、程序化解等策略手段纾解压力，带来资源消耗并未转化为实质的问题解决和治理效能提升的体制空转问题。

二是大量无理诉求涌入热线造成“资源投入无效悖论”。

前面讨论的是12345热线的运转效能问题，而这里讨论的是12345热线的治理效能问题。

对12345的另一重批判，来源于12345热线中涌现出大量不合理工单，且有愈演愈烈的趋势。调研中发现，不合理工单主要有两种类型。

一是公私不分型。

公私不分的核心逻辑是政府与个人之间责任意识边界模糊，政府成了各种小事、琐事的第一责任人，个体不再对自己的生活负责，要求政府包办代替。

如某村民院子里的树枯死，以消除安全隐患为由，打12345热线要求乡村两级上门为其砍树。最终该工单被乡镇压给了村上，村里掏了300多元出来砍树。

二是以公谋私型。

以公谋私与公私不分不同，以公谋私的核心逻辑是利用体制向政府施压，来满足自身不合情理的私人诉求。

如J村搞美丽乡村建设时，就有村民投诉施工队扰民、破坏农地等问题，而包裹在这些正式诉求背后的真实诉求，是该村民想承包家门口那一段的土方工程。

上述类型分别代表了“白嫖者”和“钉子户”两种基层治理中的不合作者群体，他们的行动本质都是对个人私利不加约束、不负责任、毫无负担地施加于政府，大量公共资源耗散在问题群众的不合理问题之上，带来基层治理负担、公私责任边界模糊化、政府权威丧失与公共规则瓦解等非预期后果。

不合作者作为乡村治理中的消极分子，是制度的冗余，不可能完全剔除。乡村有效治理的关键，并不是消除所有的不合作者，这既无必要也不可能。有效治理的关键在于形成正向的分类治理氛围，从而实现消极分子的边缘化。

这样，一方面消极分子作为边缘者，对村庄公共资源无所消耗；另一方面消极分子也以其边缘人形象对普通村民形成威慑，反过来进一步强化了公共规则。在此环境下，消极分子的存在不仅是必然冗余，也是必要冗余，在对消极分子的分类治理过程中，村庄内生的公共规则生成与强化。

当前12345热线陷入“资源悖论”的核心原因是，大量消极分子通过正式的制度途径满足个人不合理诉求，这不仅消耗了公共治理资源，还破坏了地方政府约束消极分子、维护公共规则的合法性、权威性，引发普通村民的比较、不满与模仿。于是，基于公共规则之上的“依靠积极分子，团结中间分子，边缘消极分子”的正向秩序，转变为了“突出消极分子，分化中间分子，削弱积极分子”的负向秩序。消极分子无负担、无风险地损公肥私破坏了公共规则，而公共规则的瓦解进一步激发了消极分子损公肥私的行为取向，负向循环的生成，带来了“资源投入而治理无效”的资源悖论。

由此可见，“资源悖论”的核心逻辑在于12345热线通过跨层级、高效率、低成本的制度设计，破坏了基层治理中基于分类治理形成的平衡秩序。

一方面，国家大范围地直面个体化的农民，高度的信息不对称让政府对消极分子进行分类治理的成本增加；另一方面，全响应、高效率的回应要求则让基层政府对消极分子进行分类治理的能力弱化，这又进一步带来了无理诉求的膨胀，加剧了分类治理的成本。于是，无差别、高效率地对个体诉求的满足，让大量公私不分、以公谋私的不合理诉求涌入制度通道。

对这些不合作者而言，投诉既无社会成本、也无行政负担，因此利益被满足是赚了，利益不被满足也不亏。然而与消极分子无损失相对比的，是基层政府的高耗能、高成本。

基层政府陷入两难境地：识别和剔除这些不合理诉求需要耗散大量体制资源，同时还要承担指标考核的问责压力，而满足这些不合理诉求则消耗政府的合法性权威，则带来了更多不合理诉求的涌现。在两面夹击的环境中，基层政府通过小范围摆平、大范围申诉的方式来维持基本平衡，但也让大量注意力和体制资源被耗散了在应付小部分问题人群之中。（新乡土）



## 6

【梅新育：杜特尔特被捕再次暴露国际刑事法院威胁国家安全】

今日头条文章链接：http://t.cn/A6BOcIQ7；

今日头条约稿。

文章论点：

“成功”逮捕关押杜特尔特堪称国际刑事法院的里程碑，因为国际刑事法院此前成功拘捕过前领导人的几个国家基本上都是非洲小国寡民，而菲律宾人口过亿。

该法院许多年来的许多行为实质上是在践踏国家主权，侵蚀国际社会基本秩序的基石，我国没有加入该法院是正确的，应前瞻防范国际刑事法院对中国国家利益、国家安全的威胁。

我国有必要进一步加强警惕防范国际刑事法院对我国国家安全的潜在与现实威胁，与其它国家合作、推动解散国际刑事法院的选项应该摆上桌面。俄罗斯已经向国际刑事法院及其首席检察官卡里姆·汗发出了通缉令，美国政府也已经不止一次制裁国际刑事法院及其检察官，在推动国际刑事法院改革、乃至解散方面，中、美、俄“上三常”有着相当巨大的共同利益。

## 7

莫迪：特朗普的勇气和果断深深地打动了我

在接受弗里德曼的访谈时，莫迪表示，他对特朗普总统深表钦佩，对特朗普的坚韧和爱国精神表示惊叹。

莫迪表示，即使在被枪击后，特朗普对美国的坚定不移的奉献精神仍然闪耀着光芒，特朗普的一生都献给国家，这与他自己的“美国优先，民族优先”信念如出一辙。

此外，莫迪还分享了一个小细节，即在莫迪2019年在休斯顿体育场参加集会时，莫迪建议让特朗普和莫迪自己走进人群，绕场一周这对美国总统来说是一种罕见的举动。特朗普立即同意了。

“特朗普的安保人员当时大吃一惊。这种勇气和果断深深地打动了我。”

后来，特朗普在没有笔记或助手的情况下带领莫迪参观了白宫对莫迪来说，这些时刻展示了一位非凡决心的领导人，赢得了他最大的尊重。

`印度[超话]##海外新鲜事##微博新知` http://t.cn/A6BOwJ4a

## 8

`模型时代` 可视化解释：ChatGPT这样的大语言模型，到底是如何工作的?

最近发现油管新起了一个专注于大模型技术科普的频道，叫Under The Hood。难度适中，路人友好，而且每集之间既可以互相参照，也可以独立学习。

当然，既然是科普，就讲不到强化学习、GRPO这些。我先发一下第一集，后续看情况要不要继续转译发布。

***

讲座概要：深入解析大语言模型：ChatGPT、Claude与Gemini背后的核心原理

概述

这篇文章基于Under The Hood频道的系列课程第一讲，该系列致力于帮助从AI新手到技术爱好者理解大语言模型(Large Language Models, LLMs)的工作原理。本讲座探讨了LLM的基本构造、训练流程和核心机制，从Transformer架构到自回归预测，为理解像ChatGPT、Claude和Gemini这样的现代AI系统提供了清晰的框架。讲座采用循序渐进的方式，将复杂的技术概念用简单易懂的语言呈现，特别适合想要了解AI技术内部运作但缺乏深厚技术背景的观众。

大语言模型的定义与定位

大语言模型(LLM)是一种专为自然语言处理而设计的高级机器学习模型。根据讲座中提到的维基百科定义，LLM是"一种专门用于自然语言处理的机器学习模型，可以执行语言生成等任务"。本质上，LLM是一种人工智能形式，旨在理解和生成类人文本。

讲座中形象地描述道："它是个聪明的程序，能看懂文字、会写作，还能跟你聊天说话。"这些模型不仅能回答问题，还能编写代码，甚至执行各种复杂的语言任务。然而，值得注意的是，人工智能远不止LLM，它包含众多分支领域，如机器人技术、自动化、专家系统和智能游戏系统等。

LLM位于AI技术谱系的深层，是机器学习特别是深度学习领域的产物。为理解LLM，我们需要首先掌握其在整个AI生态系统中的位置，并认识到它只是广阔AI领域中的一个专业分支。

机器学习与神经网络：LLM的技术基础

在广阔的AI领域中，机器学习是我们最关注的分支。机器学习主要分为三大类：监督学习、无监督学习和强化学习。在这些类别下存在各种各样的算法，而神经网络则是大语言模型的核心驱动力。

深度学习是机器学习的一个专业分支，它利用深度神经网络进行计算。这一领域包含了人工神经网络、卷积神经网络、循环神经网络、Transformer架构和生成对抗网络等多种技术。

对于大语言模型来说，Transformer架构是最关键的组件。讲座强调："在大语言模型中，Transformer架构扮演着核心组件的角色。"这一架构已在多个领域得到广泛应用，如Vision Transformer用于图像处理，T5用于序列转换，BERT负责自然语言处理，而类GPT模型则专注于生成文本。

Transformer架构最初由《Attention is All You Need》这篇开创性论文引入，包含两个主要部分：编码器和解码器。虽然最初为机器翻译设计，但现代LLM如GPT系列已简化为仅使用解码器部分的架构。

预测下一个词：LLM的核心功能

大语言模型能够执行众多复杂任务的秘密，实际上归结为一个简单而强大的原理：预测下一个词。讲座中明确指出："大语言模型的核心功能只有一个：预测下一个词。"

这一过程是自动递归的：当你输入"the"，模型可能预测"cat"；接着有了"the cat"，模型预测"sat"；然后是"the cat sat"，模型预测"on"。通过不断重复这一过程，模型能够生成完整的句子、段落甚至整篇文档。

"这种理解上下文的能力使模型在预测下一词时更出色。这可不是在随便猜词，而是根据句子的上下文来预测最合适的下一个词。"这种预测特性在技术上被称为自回归模型。从数学角度看，这是基于条件概率的计算，即给定先前所有词的情况下，预测下一个词的概率。

例如，给定文本"the cat sat on a"作为上下文，模型会计算下一个词的概率分布，然后基于这一分布采样得到新的词元。这种简单的机制，经过海量数据训练后，能产生令人惊叹的效果。

解码器架构：文本生成的核心引擎

解码器是大语言模型中驱动文本生成的核心引擎。在训练完成后，我们只需提供一些上下文词，模型就能生成文本。

举例来说，当我们输入"法国的首都是"这句话时，系统会将文本分解成独立的标记，送入模型的解码器处理。模型计算出最可能出现的下一个词的概率分布，然后从中选择一个作为答案，在这个例子中，答案是"巴黎"。

讲座指出："要想让模型准确预测下一个词，模型需要通过海量数据来训练。"这些训练数据涵盖了各类书籍、文献、文章、报刊、期刊、互联网内容和程序代码等，总量达数千TB，包含数万亿个词。

值得注意的是，大语言模型本质上是基于上下文预测的，这意味着它们对问题的回答方式取决于提问的形式。例如，当问"法国的首都是什么？"时，模型可能首先预测的是问号，因为这符合书写惯例；而当提示为"法国的首都是..."时，模型更可能直接预测"巴黎"。

大语言模型的训练阶段

大语言模型的训练过程通常分为几个关键阶段，每个阶段都有特定的目标和方法。

第一阶段是预训练，在这一阶段，模型通过海量原始数据学习语言的基本结构和模式。讲座解释："经过海量原始数据训练的模型，我们称之为基础模型或基座模型。这个模型能够出色地理解语言并生成连贯的文本，但还不能直接用作聊天机器人。"

第二阶段是有监督微调，这一阶段帮助模型学会遵循指令，像聊天机器人一样工作。在这个阶段，"模型会使用人工标注的精选数据集进行训练"。这些数据集按照特定的提示模板组织，模拟真实的人机对话。不同的模型提供商（如OpenAI、Meta等）可能使用不同的模板格式，但本质是一样的。

对于一些模型，还存在第三阶段训练：偏好对齐。"其核心是让模型的输出更符合人类偏好。"在这个阶段，会引入奖励模型，LLM生成高质量回答获得高分奖励，低质量回答则获得低分。模型通过这种方式逐渐学习人类的偏好，并调整其输出。

微调可以根据特定需求定制，例如让模型模仿特定角色的说话风格，或增强其在特定领域（如数学问题）的推理能力。这些特定数据集由人工标注者准备，通常包含数百万个训练样本。

大语言模型的运行机制

训练完成后的大语言模型是如何实际工作的？当用户发送请求时，系统会经过几个处理步骤。

首先，用户输入被包装在提示模板中，包含系统指令和一个用于助手回应的空白区域。然后，模型开始预测并生成回应。

由于LLM采用自回归机制，每次只能生成一个词，因此用户看到的逐字显示效果并非界面动画，而是模型真实的工作过程。"这种逐字显示的效果并非单纯的界面动画，这正是模型逐字生成的真实工作过程。"

系统会将每个新生成的词加入上下文，用于辅助生成下一个词，直到生成完整答案或遇到终止符为止。大语言模型默认不知道何时停止生成，因此在训练数据中每个样例后都加入了特殊的结束符。模型会在遇到结束符或达到预设的最大生成长度时终止生成。

最后，系统会对回复（如代码片段）进行解析和格式化，使其更易于阅读和使用。

未来学习路径

讲座最后提到了深入学习大语言模型所需了解的其他关键组件。这包括：

数据集准备和分词处理，将文本正确地转换为模型可处理的标记。

词嵌入，将词元转化为能表达语义和上下文的数值向量。

注意力机制，使模型能够识别关键词并理解上下文含义。

其他组件如前馈神经网络和残差连接。

输出层，这里会输出下一个词元的预测概率。

"理解这些基础组件很重要，对于后续学习GPT等完整语言模型架构。"这些内容将在系列的后续视频中详细介绍。

结论

大语言模型代表了人工智能领域的重大突破，它们能够理解和生成类人文本的能力源于简单而强大的原理：预测下一个词。通过海量数据训练和多阶段优化，这些模型已经能够执行从回答问题到编写代码等各种复杂任务。

虽然其核心机制看似简单，但实现这一能力需要复杂的神经网络架构（尤其是Transformer）和精心设计的训练流程。理解这些基础知识对于把握AI技术的发展方向和潜力至关重要。

作为一个观众，最让我印象深刻的是讲座中关于"预测下一个词"这一简单原理如何支撑起复杂AI系统的解释。这种"复杂中的简单性"提醒我们，有时候最强大的系统往往建立在最基本的原理之上。通过海量数据和精心训练，一个简单的预测机制能够产生如此丰富多样的语言能力，这既展示了统计学习的力量，也启示我们在技术设计中保持概念清晰和目标明确的重要性。 http://t.cn/A6BOcG6K

## 9

人上人都有一个彰显优越的心理需求，开大G的要是和劳苦大众平起平坐，那不是白开豪车了？该怎样才能实现“人人平等可我比你高贵”呢？

奢侈品名牌货这些外在物质表现都不用多说，可咱这出于稳定的需要又从来都希望人上人闷声发大财（不然太拉仇恨），问题就变成了【该怎样既显得我高贵又不那么暴发户符合咱这正确】——掌握定义权的群体，去定义什么是高贵什么是低贱，不就行了？

隔夜柠檬就是低贱，但不能明说，要打着食品安全的绝对正确来说

你反对也罢赞成也罢，“隔夜柠檬”这个概念是不是通过这波操作深入人心了？以后你再看见谁喝蜜雪是不是就有“这个人可能在喝隔夜柠檬水”的意识了？喝星巴克的是不是不费吹灰之力就获得了廉价优越，你喝隔夜我喝洋牌子，你说谁优越？

人家要的是骨子里的贵族地位，是你发自内心心悦诚服的认可。人的欲望是无止境的，习惯了依靠经济等附加条件才得到尊重，就会进一步发展到“我没有经济条件权力地位你也要尊重我”，像古代贵族那样基于血脉传承的尊重

现在制定贵族规则（比如喝蜜雪的低贱喝星巴克的高贵，吃淀粉肠的低贱吃进口肠的高贵），只是这个群体刚开始的试探罢了，接下来必将还有一波又一波的推进，不要以为这只是抽风的偶然，谁都挡不住这个群体在普通人身上找优越的那颗心

甚至有时候，中间阶层为了融入上流社会，为了看起来像个上等人为了给上层表忠心，对待底层民众会远比真正上层还要狠毒 http://t.cn/A6mAw5vM

## 10

人和人的智商差异极大。

小学，一直是全班倒数。

初中，某次考试三门课加起来考了18分。

高中，平时依然是全年级倒数。别人记单词，看一眼就记住了，哥哥比较笨，要看很多次才能记住。别人学数学，学一遍就能掌握，哥哥太笨了，要反复理解才能掌握。这么蠢的人，下场是什么？工厂拧螺丝、背水泥、抹腻子...如何改命？

初三时，考前复习，刷题，总结规律，差1分没考上高中，后来学校录取线降了10分，以超越录取线9分之姿，进入高中。

高三时，考前复习，刷题，总结规律，以超越录取线1分之姿，勉强考个本科。虽然愚钝且懒惰，但有一流的自学能力，一流心态，弱一流悟性，只要愿意，每次都能逆风翻盘。以满分天赋10分计算，哥哥大概是7分（改天写评分体系）。


这个世界，遍地都是5分以下的人，智商愚钝，自学力差，悟性差，他们该怎么办？那些聪明的人，很快就能学会知识，考试成绩很高。那些很强的人，很快就能完成工作，根本无需加班。

大量蠢货，只有补课，才能考个高分，上大学。大量弱鸡，只有加班，才能做好工作，拿高薪。

补课很贵，很累，但是至少还有机会。加班很苦，很累，但是至少还有机会。只要还在牌桌，哪怕是被强行按在牌桌，就有赢的机会。

真正的绝望是什么？禁止上桌。

禁止补课，禁止加班，

5分以下的低等人，根本无法和8分高等人竞争。自由选择，低等人想上桌，就必须内卷。如果不想学习，可以放弃补课。如果不想赚钱，可以直接躺平。


实际情况是什么？

优等人，非常厌恶奋斗逼，通过努力抹平天赋差距。

低等人，非常厌恶奋斗逼，想将同类人拉回同层次。

目前，已经禁止补课，高中强行放假。目前，禁止加班的呼声，越来越强烈。不甘被命运摆布的凡人，改变命运的难度越来越高。优势者，自身地位将得到史无前例的巩固！


何为命运？父母基因，是命。时代进程，是运。凡事都有一线生机，人类大脑有极强的可塑性，不甘被命运摆布的低分者，只要扩大阅读面，提高试错频率，多加班，胜率就能上升。996只是入门，神魔皆以血饲，我命由我不由天。

## 11

现在不少人都焦虑，担心 AI 迅速崛起后会取代很多职业，导致不知道该怎么选专业，尤其是对高考志愿迷茫的同学们来说，更是难上加难。虽然现在还不是报考季，正好在微博上讨论到这个话题，就捎带着写写我的看法，在 AI 时代，该如何选择适合自己的专业，一家之言，仅供参考。

我的观点很简单：

1. 回归初心，选择自己真正热爱的专业

不要盲目跟风去报所谓的“热门专业”，因为真正决定你能走多远、走多高的，是你对这个领域的兴趣与热情，而不是短期内的热门程度。

2. 如果实在不知道喜欢什么，那就报基础学科

比如数学、物理、化学、经济、统计等基础专业，打牢基本功之后，再根据兴趣转专业，或者读研究生时再选择适合自己的方向。基础越扎实，未来的灵活性就越高。

3. 计算机专业仍然值得考虑

如果你对计算机感兴趣，就大胆选择，不用过于担心被 AI 取代，因为未来对计算机人才的需求不会减少，只是能力要求会有所变化。

为什么大家如此担忧 AI？

很多人对 AI 的恐惧，主要是受到媒体“夸张式”宣传影响，加上自身对 AI 技术了解不足，才产生了焦虑。诚然，AI 已经改变了不少行业，也取代了一些传统工作，但更多的，AI 是提升了普通人的能力下限，同时也大幅提高了专业人士的效率。

什么是提升普通人能力的「下限」呢？

如果把专业能力比作 0 到 100 分，我们以英语翻译为例：

• 在没有 AI 时，普通人靠自己可能只能达到 30 分。

• 有了 AI 翻译工具之后，普通人借助 AI 可以轻松达到 70 分左右，这对很多日常沟通场景已经足够了。

• 但如果是法律、医学等需要极高准确性的领域，比如需要达到 90 分以上，那么 AI 就不够用了，还是得靠专业人士把关。

再以软件开发为例：

• 过去普通人根本不懂编程（0 分），现在借助 AI 工具，可以达到 30-40 分，做一些简单的网站或应用原型变得容易。

• 但稍微复杂的、专业性较高的软件开发，AI 目前还远远不够，这时专业的软件工程师就非常重要。

什么是加倍提升专业人士效率？

对于专业人士来说，AI 的帮助更加明显：

• 比如一名翻译，以前翻译一篇文章可能要几个小时，现在借助 AI，只需十几分钟对结果进行修改和润色，就能达到专业标准。

• 再如程序员，有了 AI 的协助，开发效率提升 30%以上，有些任务甚至可以翻倍，且还能降低出错概率。

有了 AI 的加成，专业人士的能力也可以横向扩展到其他相似领域。比如我是个熟练的 TypeScript 工程师，本来不懂 Python，但现在我借助 AI，可以快速用 Python 做出质量不错的项目，因为我可以复用已有的架构设计、编程思维和经验，而语言本身的差异 AI 会帮我快速弥补。

但对于完全不懂软件开发的人来说，即使使用 AI，也只是被动地接受生成结果，很难对其进一步优化或创新。

换句话说，如果你有扎实的专业基础，在 AI 时代，普通人与你这样专业人士的差距只会更大，而不是更小。

为什么计算机专业依然值得报考？

很多人质疑，“计算机专业最火，不就意味着以后竞争最激烈吗？”

其实并不是这样，热门并不一定代表竞争就会非常内卷，核心还是取决于市场需求。如果需求足够多，岗位足够丰富，即使从业人数多，也不会出现明显的过度竞争。可以预计的是，未来随着 AI 技术深入各行各业，会产生大量与计算机相关的新需求，比如 AI 工程师、数据分析师、跨领域软件开发等。

未来计算机专业的技能要求确实会变化，主要是两个方向：

1. 专业型技能：

深入学习计算机和 AI 领域的核心技术，成为顶尖技术人才。

2. 跨领域整合型技能：

不需要编程能力达到顶尖，但需要你具备某个特定领域（比如医疗、金融、教育）的专业知识，能够用计算机和 AI 技术进行行业创新或升级，像医疗领域的 AI 辅助诊断等。

此外还有一些通用技能，AI 是难以取代的，比如：

• 工程思维与解决复杂问题的能力

• 创新意识与创造力

• 管理能力，沟通协作与跨领域协调能力

这些都是人类特有的优势，即使 AI 再厉害，也无法彻底替代。

无论选哪个专业，真正重要的还是你内心的兴趣和长期坚持的毅力。

因为专业并非报了就能成为专家，而是需要投入大量时间与精力反复磨练。如果不是你真心热爱的领域，很难坚持到底。很多人只是为了薪资高、市场需求大才选择了计算机，但当行业有波动时，他们往往就会非常被动。

在 AI 时代，最不容易被取代的，是那些愿意深入思考、持续学习和不断进化的人。

选择你所爱，爱你所选，AI 时代的未来其实属于那些拥有明确目标并勇于拥抱变化的人。

希望你也能找到自己的热情所在，并在未来，和 AI 一起变得更加强大。

