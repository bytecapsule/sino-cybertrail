# 2016-08-09

## 1

Python 数据处理：Pandas 模块的 12 种实用技巧

Python 数据处理：Pandas 模块的 12 种实用技巧

原创 2016-08-08 伯乐专栏/A童鞋 Python开发者

简介

Python 正迅速成为数据科学家们更为钟爱的编程语言。形成该现状的理由非常充分：Python 提供了一种覆盖范围更为广阔的编程语言生态系统，以及具有一定计算深度且性能良好的科学计算库。如果您是 Python 初学者，建议首先看下Python 学习路线（http://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/）。

在 Python 自带的科学计算库中，Pandas 模块是最适于数据科学相关操作的工具。它与 Scikit-learn 两个模块几乎提供了数据科学家所需的全部工具。本文着重介绍了 Python 中数据处理的 12 种方法。此前的文章里也分享了一些技巧和经验，这些将有助于您提高工作效率。

在本文开始前，推荐读者首先了解一些数据挖掘（http://www.analyticsvidhya.com/blog/2015/02/data-exploration-preparation-model/）的相关代码。为使本文更易于理解，我们事先选定了一个数据集以示范相关操作和处理方法。

数据集来源：本文采用的是贷款预测问题的数据集（http://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction）。请下载该数据集并开始本文内容。

让我们开始吧

首先导入相关模块并加载数据集到 Python 环境中：

import pandas as pd

import numpy as np

data = pd.read_csv("train.csv", index_col="Loan_ID")

#1 – 布尔索引

如果需要以其它列数据值为条件过滤某一列的数据，您会怎么处理？例如建立一个列表，列表中全部为未能毕业但曾获得贷款的女性。这里可以使用布尔索引，代码如下：

data.loc[(data["Gender"]=="Female") & (data["Education"]=="Not Graduate") & (data["Loan_Status"]=="Y"),

["Gender","Education","Loan_Status"]]

更多内容请参阅：利用 Pandas 模块进行选择和索引（http://pandas.pydata.org/pandas-docs/stable/indexing.html）

#2 – Apply 函数

Apply 函数是处理数据和建立新变量的常用函数之一。在向数据框的每一行或每一列传递指定函数后，Apply 函数会返回相应的值。这个由 Apply 传入的函数可以是系统默认的或者用户自定义的。例如，在下面的例子中它可以用于查找每一行和每一列中的缺失值。

#Create a new function:

def num_missing(x):

return sum(x.isnull())

#Applying per column:

print "Missing values per column:"

print data.apply(num_missing, axis=0) #axis=0 defines that function is to be applied on each column

#Applying per row:

print "nMissing values per row:"

print data.apply(num_missing, axis=1).head() #axis=1 defines that function is to be applied on each row

这样我们就得到了所需的结果。

注：由于输出结果包含多行数据，第二个输出函数使用了 head() 函数以限定输出数据长度。在不限定输入参数时 head() 函数默认输出 5 行数据。

更多内容请参阅：Pandas 参考（apply 函数）（http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html#pandas.DataFrame.apply）

#3 – 填补缺失值

fillna() 函数可一次性完成填补功能。它可以利用所在列的均值/众数/中位数来替换该列的缺失数据。下面利用“Gender”、“Married”、和“Self_Employed”列中各自的众数值填补对应列的缺失数据。

#First we import a function to determine the mode

from scipy.stats import mode

mode(data['Gender'])

输出结果为：ModeResult(mode=array([‘Male’], dtype=object), count=array([489]))

输出结果返回了众数值和对应次数。需要记住的是由于可能存在多个高频出现的重复数据，因此众数可以是一个数组。通常默认使用第一个众数值：

mode(data['Gender']).mode[0]

现在可以进行缺失数据值填补并利用#2方法进行检查。

#Impute the values:

data['Gender'].fillna(mode(data['Gender']).mode[0], inplace=True)

data['Married'].fillna(mode(data['Married']).mode[0], inplace=True)

data['Self_Employed'].fillna(mode(data['Self_Employed']).mode[0], inplace=True)

`Now check the `missing values again to confirm:

print data.apply(num_missing, axis=0)

至此，可以确定缺失值已经被填补。请注意，上述方法是最基本的填补方法。包括缺失值建模，用分组平均数（均值/众数/中位数）填补在内的其他复杂方法将在接下来的文章中进行介绍。

更多内容请参阅：Pandas 参考（ fillna 函数）（http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html#pandas.DataFrame.fillna）

#4 – 数据透视表

Pandas 可建立 MS Excel 类型的数据透视表。例如在下文的代码段里，关键列“LoanAmount” 存在缺失值。我们可以根据“Gender”，“Married”和“Self_Employed”分组后的平均金额来替换。 “LoanAmount”的各组均值可由如下方法确定：

#Determine pivot table

impute_grps = data.pivot_table(values=["LoanAmount"], index=["Gender","Married","Self_Employed"], aggfunc=np.mean)

print impute_grps

更多内容请参阅：Pandas 参考（数据透视表）（http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pivot_table.html#pandas.DataFrame.pivot_table）

#5 – 复合索引

如果您注意观察#3计算的输出内容，会发现它有一个奇怪的性质。即每个索引均由三个数值的组合构成，称为复合索引。它有助于运算操作的快速进行。

从#3的例子继续开始，已知每个分组数据值但还未进行数据填补。具体的填补方式可结合此前学到的多个技巧来完成。

#iterate only through rows with missing LoanAmount

for i,row in data.loc[data['LoanAmount'].isnull(),:].iterrows():

ind = tuple([row['Gender'],row['Married'],row['Self_Employed']])

data.loc[i,'LoanAmount'] = impute_grps.loc[ind].values[0]

`Now check the `missing values again to confirm:

print data.apply(num_missing, axis=0)

Note:

注：

1. 多值索引需要在 loc 语句中使用用于定义索引分组的元组结构。该元组会在函数中使用。

2. 应使用后缀 .values[0] 以避免潜在的错误。因为默认情况下复合索引返回的 Series 元素索引顺序与所在的数据框架（dataframe）不一致。在此条件下直接赋值会产生错误。

#6 – Crosstab 函数

该函数用于获取数据的初始印象（直观视图），从而验证一些基本假设。例如在本例中，“Credit_History”被认为会显著影响贷款状态。这个假设可以通过如下代码生成的交叉表进行验证：

pd.crosstab(data["Credit_History"],data["Loan_Status"],margins=True)

以上这些都是绝对值。但百分比形式能获得更为直观的数据结果。使用 apply 函数可实现该功能：

def percConvert(ser):

return ser/float(ser[-1])

pd.crosstab(data["Credit_History"],data["Loan_Status"],margins=True).apply(percConvert, axis=1)

现在可以证明：与仅占9%的无信用记录人群相比，占比为80%的有信用记录人群获得贷款的几率会更高。

但这并不是全部的数据结果，其中还包含了一个有趣的内容。既然已知有信用记录非常重要，如果利用信用记录情况进行贷款预测会如何？其中，预测有信用记录的人的获得贷款状态为 Y，否则为 N。令人吃惊的是在614次测试中，共正确预测了 82+378=460 次，正确率高达75%！

如果您正好奇为什么我们需要统计模型，我一点儿也不会责怪您。但是请相信，提高预测精度是一项非常具有挑战性的任务，哪怕仅仅是在上述预测结果的基础上提高0.001%的预测精度也是如此。您会接受这个挑战吗？

注：75% 是对本文的训练数据集而言。测试数据集的结果将会有所不同，但也非常接近。同样地，希望通过这个例子能让大家明白为什么仅仅提高0.05%的预测精度就可在Kaggle排行榜中排名跃升500位。

更多内容请参阅：Pandas 模块参考（ crosstab 函数）（http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.crosstab.html）

#7 – 合并数据框（DataFrames）

当有来自不同数据源的信息需要收集整理时，合并数据框就变成了一项必不可少的基本操作。考虑一个假设的情况，即不同类型的房产有不同的均价（单位：INR / 平方米）。定义数据框如下：

prop_rates = pd.DataFrame([1000, 5000, 12000], index=['Rural','Semiurban','Urban'],columns=['rates'])

prop_rates

现在可将上述信息与原始数据框合并如下：

data_merged = data.merge(right=prop_rates, how='inner',left_on='Property_Area',right_index=True, sort=False)

data_merged.pivot_table(values='Credit_History',index=['Property_Area','rates'], aggfunc=len)

上述透视表验证了合并操作成功。需要注意的是由于上述代码仅对数据值进行简单计算，因此‘values’参数在本例中是一个独立内容，与上下文无关。

更多内容请参阅：Pandas 参考（ merge 函数）（http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html#pandas.DataFrame.merge）

#8 – 排列数据框架（DataFrames）

Pandas 允许基于多列数据进行简单排列。具体实现如下：

data_sorted = data.sort_values(['ApplicantIncome','CoapplicantIncome'], ascending=False)

data_sorted[['ApplicantIncome','CoapplicantIncome']].head(10)

注：Pandas模块中的“sort”函数现已不再使用，应用“sort_values”函数进行代替。

更多内容请参阅：Pandas 参考（ sort_values 函数）（http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html#pandas.DataFrame.sort_values）

#9 – 绘图（Boxplot 和 Histogram 函数）

许多人也许并没有意识到 Pandas 模块中的 boxplots 和 histograms 函数可以用于直接绘图，此时没有必要再单独调用 matplotlib 模块。一行命令即可完成相关功能。例如，如果想通过 Loan_Status 比较 ApplicantIncome 的分布情况，则实现代码如下：

import matplotlib.pyplot as plt

%matplotlib inline

data.boxplot(column="ApplicantIncome",by="Loan_Status")

data.hist(column="ApplicantIncome",by="Loan_Status",bins=30)

上图的数据结果表明，由于获得贷款人群和未获得贷款人群数没有明显的收入差距，因此个人收入水平高低并非是否能获得贷款的主要决定因素。

更多内容请参阅：Pandas 参考（ hist 函数）|（ boxplot 函数）（http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.boxplot.html#pandas.DataFrame.boxplot）

#10 – 使用 Cut 函数进行分箱

有时将数值数据聚合在一起会更有意义。例如，如果我们要根据一天中的某个时间段（单位：分钟）建立交通流量模型模型（以路上的汽车为统计目标）。与具体的分钟数相比，对于交通流量预测而言一天中的具体时间段则更为重要，如“早上”、 “下午”、“傍晚”、“夜晚”、“深夜（Late Night）”。以这种方式建立交通流量模型则更为直观且避免了过拟合情况的发生。

下面的例子中定义了一个简单的可重用函数，该函数可以非常轻松地实现任意变量的分箱功能。

#Binning:

def binning(col, cut_points, labels=None):

#Define min and max values:

minval = col.min()

maxval = col.max()

#create list by adding min and max to cut_points

break_points = [minval] + cut_points + [maxval]

#if no labels provided, use default labels 0 ... (n-1)

if not labels:

labels = range(len(cut_points)+1)

#Binning using cut function of pandas

colBin = pd.cut(col,bins=break_points,labels=labels,include_lowest=True)

return colBin

#Binning age:

cut_points = [90,140,190]

labels = ["low","medium","high","very high"]

data["LoanAmount_Bin"] = binning(data["LoanAmount"], cut_points, labels)

print pd.value_counts(data["LoanAmount_Bin"], sort=False)

更多内容请参阅：Pandas 参考（ cut 函数）（http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.cut.html）

#11 – 为名义变量编码

通常我们会遇到需要对名义变量进行分类的情况。可能的原因如下：

1. 一些算法（如逻辑回归算法）要求输入参数全部为数字。因此名义变量多需要编码为0, 1….(n-1)。

2. 有时同一种分类可以表示为两种形式。如温度可能被记录为“高（High）”、“中（Medium）”、“低（Low）”、“高（H）”、“低（low）”。在这里，“高（High）”和“高（H）”都表示同一种分类。类似地在“低（Low）”和“低（low）”的表示方法中仅存在大小写的区别。但 python 将会将它们视为不同的温度水平。

3.一些分类的出现频率可能较低，因此将这些分类归为一类不失为一个好主意。

下面的例子中定义了一个通用函数，该函数使用字典作为输入，并利用 Pandas 模块的‘replace’函数对字典值进行编码。

#Define a generic function using Pandas replace function

def coding(col, codeDict):

colCoded = pd.Series(col, copy=True)

for key, value in codeDict.items():

colCoded.replace(key, value, inplace=True)

return colCoded

#Coding LoanStatus as Y=1, N=0:

print 'Before Coding:'

print pd.value_counts(data["Loan_Status"])

data["Loan_Status_Coded"] = coding(data["Loan_Status"], {'N':0,'Y':1})

print 'nAfter Coding:'

print pd.value_counts(data["Loan_Status_Coded"])

编码前后的计数结果一致，证明编码正确。

更多内容请参阅：Pandas 参考（ replace 函数）（http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html#pandas.DataFrame.replace）

#12 – 对数据框的行数据进行迭代

这个操作不经常使用。但您也不希望被这个问题卡住，对吧？有时需要利用 for 循环对所有行数据进行迭代。例如一个常见的问题即是 Python 中变量的不当处理。通常发生在如下情况：

1.带有数字的名义变量被认为是数值数据。

2.由于数据错误，带有字符的数值变量输入行数据中时被认为是分类变量。

因此手动定义列数据类型会是一个不错的主意。如果检查所有列数据的数据类型：

#Check current type:

data.dtypes

会看到名义变量 Credit_History 被显示为 float 类型。解决这种问题的一个好方法即是创建一个包含列名和对应类型的 csv 文件。这样就可以定义一个通用函数来读取文件，并指定列数据的类型。例如在下面的例子里建立了一个 csv 文件“datatypes.csv”。

#Load the file:

colTypes = pd.read_csv('datatypes.csv')

print colTypes

加载数据后可以对每一行进行迭代，并利用‘type’列中的数据内容确定‘feature’列中对应变量名的数据类型。

#Iterate through each row and assign variable type.

#Note: astype is used to assign types

for i, row in colTypes.iterrows():#i: dataframe index; row: each row in series format

if row['type']=="categorical":

data[row['feature']]=data[row['feature']].astype(np.object)

elif row['type']=="continuous":

data[row['feature']]=data[row['feature']].astype(np.float)

print data.dtypes

现在 credit history 列被定义为了 ‘object’ 类型，该类型即为 Pandas 模块中用于表示名义变量的数据类型。

更多内容请参阅：Pandas 参考（ iterrows 函数）（http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html#pandas.DataFrame.iterrows）

结语

在这篇文章中，我们介绍了 Pandas 的多个函数，这些函数使得我们数据挖掘和特征工程上更加轻松。同时我们定义了一些可重用的通用函数，以在处理不同的数据集时可以获得类似目标结果。

## 2

10 天 100 小时学数据科学，我推荐你这样学！

10 天 100 小时学数据科学，我推荐你这样学！

原创 2016-07-14 伯乐专栏/李霄逍 Python开发者

哥们我太羡慕你了，不是谁都有像你这样的机会的。

10天100小时的学习时间里应该分配给尽量多样化的知识。这也算是一大笔投资，所以应当严肃对待，根据学出来的结果可以把拿到实习 offer 之类的当做目标。

说实话，我不觉得学习在线课程对你这次的学习有什么用，上课也不过就能给你点成就感罢了。如果你有不错的数学和编程基础，那么直接上手写代码、跑数据、看结果，比上在线课程有意思得多。那可是一百个小时呀！如果你真能在接下来十天的一百个小时里尽自己最大的努力学习，就能学到世界级专家所需知识的百分之一。时间紧任务重，你，准备好了嘛？

库和算法的内部实现之类的将来有得是时间学，你现在还用不上。我下面列出的任务量巨大，需要你全身心投入，目标是让你能够广泛地接触到这个领域的一些主要工具。

第一天和第二天

下载 StackExchange 的公开数据：下载链接(https://archive.org/details/stackexchange)（需梯子）

处理数据需要关系数据库管理系统，第一天大概要干这些：

安装 MySQL 配置好后把上面下载的数据导入数据库阅读 SQL 基础知识。花点时间做几个小练习题来熟悉数据操作。比如说，写个能够抽取所有满足下面条件的提问的脚本：提问是关于 Python 和 SQL的、回答多于三个、最佳回答的作者在这两个主题下拥有大于十个被选中的回答。你有可能会发现脚本性能有问题。阅读 SQL 索引知识，了解哈希和排序等。修改上面的脚本让它能立刻马上跑出结果。写个能处理上面查询语句的 Python 类。这就需要学习 Python MySQL driver。你需要一个工具，能帮助你从数据库中抽取数据并且能把它们以比较方便的形式呈现出来。

虽不清楚题主底子怎么样，但我觉得上面的任务即使对菜鸟来说也是完全能够完成的。你只要有些基础的 Python 知识就足够了。

第二天可以用来了解用 pandas 来读取数据，以及用 numpy 来对数值型数据进行操作。这些库的文档看起来页数多很吓人，但不用都读。只要学会导入 CSV 文件、添加提取数据列、合并两个数据库这些操作就行了。

第三天

虽然实际工作中通常都是在整个数据库上做查询，但学习如何在小量级的数据做操作并且得到有意义的结果也是相当重要的。比如说，可以试着从整个数据集中自己随机抽取一些数据，然后把它们的得分的分布和整个数据集上的得分分布做比较。

现在可以再进一步了。虽然手握整个 Stack Exchange 的数据库，但是因为我对 StackOverflow 里面的数据比较熟悉一点，下面只会用到 StackOverflow 的数据。我能想到的一个有意思的练习：根据时间看编程语言的流行程度。

为什么这个练习有用呢？

如何抽取与仅仅编程语言本身有关的提问而不抽取只与技术有关的提问呢？（比如我想要和 Python 语法有关的提问，而不想要那种问如何在 Django 中用 MangoDB 的提问）提问的标签数量非常大，为了最后将结果可视化需要谨慎挑选输入数据可以学习到至少一个可视化框架可以生成很多美图

会做上面的例子，自然而然就可以探寻更多数据里有意思的属性了。学会问问题是一个很重要的能力。

第四天和第五天

数据科学家曾被评为“21 世纪最性感的职业”。你知道还有什么很性感么？图论哦！

问题标签之间是如何联系在一起的？是否能仅仅用 Stack Overflow 上的回答就构建一个和技术有关的图？该选哪个标准来计算两个标签有多相似？图的可视化该如何做？试 Gephi 了吗？

都做好了之后，需要给上面生成的图添加描述。仅仅一张图本身能提供的价值有限，你需要一直盯着它看，直到理解它背后所代表的意义。

学习聚类算法 （至少要学 k-means 和 DBSCAN）和 K 近邻算法。要是愿意钻研的话，还可以试试各种图论算法，算图的各项指标。建议使用 networkx 这个库和 scikit-learn 库里的一些相关部分，这些库能大大简化任务的难度。

做这个有什么用？

可以接触到不同格式的数据，比如 CSV， GEPHI， 边的集合等。K-means 是一个有用的算法，学了不吃亏，以后用得上研究数据的时候，发掘有意义的聚类是最重要的任务之一

可以根据自己的情况在这两天之中分配任务。我会推荐头一天玩玩 networkx 和 Gephi。第二天我会做聚类分析，因为做聚类的时候会有一些有挑战的问题，比如你需要思考：到底该用怎样的向量来表示问题标签才能保留它们之间的距离呢？！

第六天

到了第六天应该已经基本明白数据库部分了。然而文本还没有接触过，只会算字数不算（此处有冷冷的一语双关）。

今天应该用来简单学点文本分析。学学潜在语义索引就够了，所有需要的东西在 scikit-learn 库里都有，还需要用一些 SQL。一般的流程是：

选出想用的数据用 scikit 来抽取文本特征 （建议用 scikit 里的 TF-IDF Vectorizer）给文本加标签。你可以做一个简单的练习：根据回答的文本来预测它会得多少分。这里文本的得分就是它的标签，TF-IDF 可以作为特征向量。

最好是能用 numpy 格式来为每个假设准备一个数据集。比如：

一个用来预测回答的得分对回答按主题分类 （可以选择二十种编程语言的一些回答作为样本）

一定要注意搜集到的数据集要清理干净，你要确切地知道里面都有些什么。说起来简单做起来难。

第七、八、九天

前一天中已经得到了干净的数据集。假设一个用于分类一个用于预测（在第五天时已经学过它们的区别了）。在这几天（译者注：原文写的第五天应是笔误）该集中学习回归模型了。scikit 库里提供了很全面的工具。应该上手试试下面提到的方法里至少三种：

线性模型。线性模型的种类浩如烟海。首先要比较它们的性能，然后读读最好的线性模型的相关知识，了解不同模型的区别。提示：好好学学 ElasticNet 回归。如果你数学还行的话可以读读 Bishop 的《模式识别与机器学习》 一书。书里很好地讲解了 ElasticNet 回归模型好用的原因。如果没时间的话可以不看。回归树。KNN 回归。KNN 通常很好用，不要瞧不起这些简单的方法。集成学习模型如随机森林和自适应增强学习。

学习的主要目标并不是立刻变成这些算法的专家，而应该先跑起来代码，好用了之后然后再问问题。

同样的方法也适用于分类问题。思考应该用什么指标才能衡量结果的好坏。假设要建立一个给新闻排序的智能信息平台，该如何评估它的好坏呢？

对所有的模型做交叉验证是必不可少的。阅读 k 折（k-fold）交叉验证有关内容，研究如何用 scikit 来做 k 折交叉验证，然后对所有你之前建立过的模型都做一遍交叉验证。

第十天

既然你想做一个数据科学家，这次经历不能落下它最有意思的部分，那就是展示结果。

无论你想选什么样的形式来展示它们都无所谓（将来很难有这样自由选择展示形式的机会了）。无论是半学术论文形式、PPT 展示、博客文章，还是一个手机 app 都可以，任君选择。把你的故事分享给大家。写写你在数据集里发现了什么、都做了些什么假设，分析假设能否成立的原因、简单描述下用到的算法、用简洁明了的形式来展示交叉验证的结果等，并且一定要多多放些图表。

这部分无论做到什么程度都不算用力过猛。我保证，如果你真能做一个好的展示并且展示给自己的伯乐看，入门 offer 指日可待。

【补注】：Roman Trusov 的这个回答有 760+ 顶，并且有不少评价非常高的评论。伯小乐摘编几个：

Muhammed Hussain：我没有回答题主的问题，但看到这个回复后，我决定从工作中抽出 10 天来学习。

Sethu Iyer：哇！从没想过 10 天能如此高效！

TEJAS SARMA：现在我知道暑假头 10 天能做什么了。

Parisa Rai：读完这个回答，我是多么希望自己还是一个初学者呀。我认为，除了那些只有 10 天空闲时间的童鞋，每个初学者都可以按答主的建议来。

Anastasia Kukanova：你知道还有什么很性感么？这个回答！

## 3

你应该掌握的七种回归技术

链接：www.iteye.com/news/30875

什么是回归分析？

回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。例如，司机的鲁莽驾驶与道路交通 事 故数量之间的关系，最好的研究方法就是回归。

回归分析是建模和分析数据的重要工具。在这里，我们使用曲线/线来拟合这些数据点，在这种方式下，从曲线或线到数据点的距离差异最小。我会在接下来的部分详细解释这一点。

我们为什么使用回归分析？

如上所述，回归分析估计了两个或多个变量之间的关系。下面，让我们举一个简单的例子来理解它：

比如说，在当前的经济条件下，你要估计一家公司的销售额增长情况。现在，你有公司最新的数据，这些数据显示出销售额增长大约是经济增长的2.5倍。那么使用回归分析，我们就可以根据当前和过去的信息来预测未来公司的销售情况。

使用回归分析的好处良多。具体如下：

1.它表明自变量和因变量之间的显著关系；

2.它表明多个自变量对一个因变量的影响强度。

回归分析也允许我们去比较那些衡量不同尺度的变量之间的相互影响，如价格变动与促销活动数量之间联系。这些有利于帮助市场研究人员，数据分析人员以及数据科学家排除并估计出一组最佳的变量，用来构建预测模型。

我们有多少种回归技术？

有各种各样的回归技术用于预测。这些技术主要有三个度量（自变量的个数，因变量的类型以及回归线的形状）。我们将在下面的部分详细讨论它们。

对于那些有创意的人，如果你觉得有必要使用上面这些参数的一个组合，你甚至可以创造出一个没有被使用过的回归模型。但在你开始之前，先了解如下最常用的回归方法：

1. Linear Regression线性回归

它是最为人熟知的建模技术之一。线性回归通常是人们在学习预测模型时首选的技术之一。在这种技术中，因变量是连续的，自变量可以是连续的也可以是离散的，回归线的性质是线性的。

线性回归使用最佳的拟合直线（也就是回归线）在因变量（Y）和一个或多个自变量（X）之间建立一种关系。

用一个方程式来表示它，即Y=a+b*X + e，其中a表示截距，b表示直线的斜率，e是误差项。这个方程可以根据给定的预测变量（s）来预测目标变量的值。

一元线性回归和多元线性回归的区别在于，多元线性回归有（>1）个自变量，而一元线性回归通常只有1个自变量。现在的问题是“我们如何得到一个最佳的拟合线呢？”。

如何获得最佳拟合线（a和b的值）？

这个问题可以使用最小二乘法轻松地完成。最小二乘法也是用于拟合回归线最常用的方法。对于观测数据，它通过最小化每个数据点到线的垂直偏差平方和来计算最佳拟合线。因为在相加时，偏差先平方，所以正值和负值没有抵消。

我们可以使用R-square指标来评估模型性能。想了解这些指标的详细信息，可以阅读：模型性能指标Part 1,Part 2 .

要点：

自变量与因变量之间必须有线性关系多元回归存在多重共线性，自相关性和异方差性。线性回归对异常值非常敏感。它会严重影响回归线，最终影响预测值。多重共线性会增加系数估计值的方差，使得在模型轻微变化下，估计非常敏感。结果就是系数估计值不稳定在多个自变量的情况下，我们可以使用向前选择法，向后剔除法和逐步筛选法来选择最重要的自变量。

2.Logistic Regression逻辑回归

逻辑回归是用来计算“事件=Success”和“事件=Failure”的概率。当因变量的类型属于二元（1 / 0，真/假，是/否）变量时，我们就应该使用逻辑回归。这里，Y的值从0到1，它可以用下方程表示。

odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence

ln(odds) = ln(p/(1-p))

logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk

上述式子中，p表述具有某个特征的概率。你应该会问这样一个问题：“我们为什么要在公式中使用对数log呢？”。

因为在这里我们使用的是的二项分布（因变量），我们需要选择一个对于这个分布最佳的连结函数。它就是Logit函数。在上述方程中，通过观测样本的极大似然估计值来选择参数，而不是最小化平方和误差（如在普通回归使用的）。

要点：

它广泛的用于分类问题。逻辑回归不要求自变量和因变量是线性关系。它可以处理各种类型的关系，因为它对预测的相对风险指数OR使用了一个非线性的log转换。为了避免过拟合和欠拟合，我们应该包括所有重要的变量。有一个很好的方法来确保这种情况，就是使用逐步筛选方法来估计逻辑回归。它需要大的样本量，因为在样本数量较少的情况下，极大似然估计的效果比普通的最小二乘法差。自变量不应该相互关联的，即不具有多重共线性。然而，在分析和建模中，我们可以选择包含分类变量相互作用的影响。如果因变量的值是定序变量，则称它为序逻辑回归。如果因变量是多类的话，则称它为多元逻辑回归。

3. Polynomial Regression多项式回归

对于一个回归方程，如果自变量的指数大于1，那么它就是多项式回归方程。如下方程所示：

y=a+b*x^2

在这种回归技术中，最佳拟合线不是直线。而是一个用于拟合数据点的曲线。

重点：

虽然会有一个诱导可以拟合一个高次多项式并得到较低的错误，但这可能会导致过拟合。你需要经常画出关系图来查看拟合情况，并且专注于保证拟合合理，既没有过拟合又没有欠拟合。下面是一个图例，可以帮助理解：

明显地向两端寻找曲线点，看看这些形状和趋势是否有意义。更高次的多项式最后可能产生怪异的推断结果。

4. Stepwise Regression逐步回归

在处理多个自变量时，我们可以使用这种形式的回归。在这种技术中，自变量的选择是在一个自动的过程中完成的，其中包括非人为操作。

这一壮举是通过观察统计的值，如R-square，t-stats和AIC指标，来识别重要的变量。逐步回归通过同时添加/删除基于指定标准的协变量来拟合模型。下面列出了一些最常用的逐步回归方法：

标准逐步回归法做两件事情。即增加和删除每个步骤所需的预测。向前选择法从模型中最显著的预测开始，然后为每一步添加变量。向后剔除法与模型的所有预测同时开始，然后在每一步消除最小显着性的变量。

这种建模技术的目的是使用最少的预测变量数来最大化预测能力。这也是处理高维数据集的方法之一。

5. Ridge Regression岭回归

岭回归分析是一种用于存在多重共线性（自变量高度相关）数据的技术。在多重共线性情况下，尽管最小二乘法（OLS）对每个变量很公平，但它们的差异很大，使得观测值偏移并远离真实值。岭回归通过给回归估计上增加一个偏差度，来降低标准误差。

上面，我们看到了线性回归方程。还记得吗？它可以表示为：

y=a+ b*x

这个方程也有一个误差项。完整的方程是：

y=a+b*x+e (error term), [error term is the value needed to correct for a prediction error between the observed and predicted value]

=> y=a+y= a+ b1x1+ b2x2+....+e, for multiple independent variables.

在一个线性方程中，预测误差可以分解为2个子分量。一个是偏差，一个是方差。预测错误可能会由这两个分量或者这两个中的任何一个造成。在这里，我们将讨论由方差所造成的有关误差。

岭回归通过收缩参数λ（lambda）解决多重共线性问题。看下面的公式

在这个公式中，有两个组成部分。第一个是最小二乘项，另一个是β2（β-平方）的λ倍，其中β是相关系数。为了收缩参数把它添加到最小二乘项中以得到一个非常低的方差。

要点：

除常数项以外，这种回归的假设与最小二乘回归类似；它收缩了相关系数的值，但没有达到零，这表明它没有特征选择功能这是一个正则化方法，并且使用的是L2正则化。

6. Lasso Regression套索回归

它类似于岭回归，Lasso （Least Absolute Shrinkage and Selection Operator）也会惩罚回归系数的绝对值大小。此外，它能够减少变化程度并提高线性回归模型的精度。看看下面的公式：

Lasso 回归与Ridge回归有一点不同，它使用的惩罚函数是绝对值，而不是平方。这导致惩罚（或等于约束估计的绝对值之和）值使一些参数估计结果等于零。使用惩罚值越大，进一步估计会使得缩小值趋近于零。这将导致我们要从给定的n个变量中选择变量。

要点：

除常数项以外，这种回归的假设与最小二乘回归类似；它收缩系数接近零（等于零），这确实有助于特征选择；这是一个正则化方法，使用的是L1正则化；

如果预测的一组变量是高度相关的，Lasso 会选出其中一个变量并且将其它的收缩为零。

7.ElasticNet回归

ElasticNet是Lasso和Ridge回归技术的混合体。它使用L1来训练并且L2优先作为正则化矩阵。当有多个相关的特征时，ElasticNet是很有用的。Lasso 会随机挑选他们其中的一个，而ElasticNet则会选择两个。

Lasso和Ridge之间的实际的优点是，它允许ElasticNet继承循环状态下Ridge的一些稳定性。

要点：

在高度相关变量的情况下，它会产生群体效应；选择变量的数目没有限制；它可以承受双重收缩。

除了这7个最常用的回归技术，你也可以看看其他模型，如Bayesian、Ecological和Robust回归。

如何正确选择回归模型？

当你只知道一个或两个技术时，生活往往很简单。我知道的一个培训机构告诉他们的学生，如果结果是连续的，就使用线性回归。如果是二元的，就使用逻辑回归！然而，在我们的处理中，可选择的越多，选择正确的一个就越难。类似的情况下也发生在回归模型中。

在多类回归模型中，基于自变量和因变量的类型，数据的维数以及数据的其它基本特征的情况下，选择最合适的技术非常重要。以下是你要选择正确的回归模型的关键因素：

数据探索是构建预测模型的必然组成部分。在选择合适的模型时，比如识别变量的关系和影响时，它应该首选的一步。比较适合于不同模型的优点，我们可以分析不同的指标参数，如统计意义的参数，R-square，Adjusted R-square，AIC，BIC以及误差项，另一个是Mallows’ Cp准则。这个主要是通过将模型与所有可能的子模型进行对比（或谨慎选择他们），检查在你的模型中可能出现的偏差。交叉验证是评估预测模型最好额方法。在这里，将你的数据集分成两份（一份做训练和一份做验证）。使用观测值和预测值之间的一个简单均方差来衡量你的预测精度。如果你的数据集是多个混合变量，那么你就不应该选择自动模型选择方法，因为你应该不想在同一时间把所有变量放在同一个模型中。它也将取决于你的目的。可能会出现这样的情况，一个不太强大的模型与具有高度[*]统计学意义的模型相比，更易于实现。

回归正则化方法（Lasso，Ridge和ElasticNet）在高维和数据集变量之间多重共线性情况下运行良好。

