# 2018-01-15

## 1

从线性回归到无监督学习，数据科学家需要掌握的十大统计技术

2017-11-18机器之心

选自KDnuggets

作者：James Le

机器之心编译

参与：路雪、刘晓坤、蒋思源

「数据科学家比程序员擅长统计，比统计学家擅长编程。」本文介绍了数据科学家需要掌握的十大统计技术，包括线性回归、分类、重采样、降维、无监督学习等。

不管你对数据科学持什么态度，都不可能忽略分析、组织和梳理数据的重要性。Glassdoor 网站根据大量雇主和员工的反馈数据制作了「美国最好的 25 个职位」榜单，其中第一名就是数据科学家。尽管排名已经顶尖了，但数据科学家的工作内容一定不会就此止步。随着深度学习等技术越来越普遍、深度学习等热门领域越来越受到研究者和工程师以及雇佣他们的企业的关注，数据科学家继续走在创新和技术进步的前沿。

尽管具备强大的编程能力非常重要，但数据科学不全关于软件工程（实际上，只要熟悉 Python 就足以满足编程的需求）。数据科学家需要同时具备编程、统计学和批判思维能力。正如 Josh Wills 所说：「数据科学家比程序员擅长统计学，比统计学家擅长编程。」我自己认识很多软件工程师希望转型成为数据科学家，但是他们盲目地使用 TensorFlow 或 Apache Spark 等机器学习框架处理数据，而没有全面理解其背后的统计学理论知识。因此他们需要系统地研究统计机器学习，该学科脱胎于统计学和泛函分析，并结合了信息论、最优化理论和线性代数等多门学科。

为什么学习统计学习？理解不同技术背后的理念非常重要，它可以帮助你了解如何使用以及什么时候使用。同时，准确评估一种方法的性能也非常重要，因为它能告诉我们某种方法在特定问题上的表现。此外，统计学习也是一个很有意思的研究领域，在科学、工业和金融领域都有重要的应用。最后，统计学习是训练现代数据科学家的基础组成部分。统计学习方法的经典研究主题包括：线性回归模型感知机k 近邻法朴素贝叶斯法决策树Logistic 回归于最大熵模型支持向量机提升方法EM 算法隐马尔可夫模型条件随机场

之后我将介绍 10 项统计技术，帮助数据科学家更加高效地处理大数据集的统计技术。在此之前，我想先厘清统计学习和机器学习的区别：机器学习是偏向人工智能的分支。统计学习方法是偏向统计学的分支。机器学习更侧重大规模应用和预测准确率。统计学系侧重模型及其可解释性，以及精度和不确定性。二者之间的区别越来越模糊。

1. 线性回归

在统计学中，线性回归通过拟合因变量和自变量之间的最佳线性关系来预测目标变量。最佳拟合通过尽量缩小预测的线性表达式和实际观察结果间的距离总和来实现。没有其他位置比该形状生成的错误更少，从这个角度来看，该形状的拟合是「最佳」。线性回归的两个主要类型是简单线性回归和多元线性回归。

简单线性回归使用一个自变量通过拟合最佳线性关系来预测因变量的变化情况。多元线性回归使用多个自变量通过拟合最佳线性关系来预测因变量的变化趋势。

任意选择两个日常使用且相关的物体。比如，我有过去三年月支出、月收入和每月旅行次数的数据。现在我需要回答以下问题：我下一年月支出是多少？哪个因素（月收入或每月旅行次数）在决定月支出方面更重要？月收入和每月旅行次数与月支出之间是什么关系？

2. 分类

分类是一种数据挖掘技术，为数据分配类别以帮助进行更准确的预测和分析。分类是一种高效分析大型数据集的方法，两种主要的分类技术是：logistic 回归和判别分析（Discriminant Analysis）。

logistic 回归是适合在因变量为二元类别的回归分析。和所有回归分析一样，logistic 回归是一种预测性分析。logistic 回归用于描述数据，并解释二元因变量和一或多个描述事物特征的自变量之间的关系。logistic 回归可以检测的问题类型如下：体重每超出标准体重一磅或每天每抽一包烟对得肺癌概率（是或否）的影响。卡路里摄入、脂肪摄入和年龄对心脏病是否有影响（是或否）？

在判别分析中，两个或多个集合和簇等可作为先验类别，然后根据度量的特征把一个或多个新的观察结果分类成已知的类别。判别分析对每个对应类中的预测器分布 X 分别进行建模，然后使用贝叶斯定理将其转换成根据 X 的值评估对应类别的概率。此类模型可以是线性判别分析（Linear Discriminant Analysis），也可以是二次判别分析（Quadratic Discriminant Analysis）。线性判别分析（LDA）：为每个观察结果计算「判别值」来对它所处的响应变量类进行分类。这些分值可以通过找到自变量的线性连接来获得。它假设每个类别的观察结果都从多变量高斯分布中获取，预测器变量的协方差在响应变量 Y 的所有 k 级别中都很普遍。二次判别分析（QDA）：提供另外一种方法。和 LDA 类似，QDA 假设 Y 每个类别的观察结果都从高斯分布中获取。但是，与 LDA 不同的是，QDA 假设每个类别具备自己的协方差矩阵。也就是说，预测器变量在 Y 的所有 k 级别中不是普遍的。

3. 重采样方法

重采样方法（Resampling）包括从原始数据样本中提取重复样本。这是一种统计推断的非参数方法。即，重采样不使用通用分布来逼近地计算概率 p 的值。

重采样基于实际数据生成一个独特的采样分布。它使用经验性方法，而不是分析方法，来生成该采样分布。重采样基于数据所有可能结果的无偏样本获取无偏估计。为了理解重采样的概念，你应该先了解自助法（Bootstrapping）和交叉验证（Cross-Validation）：

自助法（Bootstrapping）适用于多种情况，如验证预测性模型的性能、集成方法、偏差估计和模型方差。它通过在原始数据中执行有放回取样而进行数据采样，使用「未被选中」的数据点作为测试样例。我们可以多次执行该操作，然后计算平均值作为模型性能的估计。

交叉验证用于验证模型性能，通过将训练数据分成 k 部分来执行。我们将 k-1 部分作为训练集，「留出」的部分作为测试集。将该步骤重复 k 次，最后取 k 次分值的平均值作为性能估计。

通常对于线性模型而言，普通最小二乘法是拟合数据时主要的标准。下面 3 个方法可以提供更好的预测准确率和模型可解释性。

4. 子集选择

该方法将挑选 p 个预测因子的一个子集，并且我们相信该子集和所需要解决的问题十分相关，然后我们就能使用该子集特征和最小二乘法拟合模型。

最佳子集的选择：我们可以为 p 个预测因子的每个组合拟合单独的 OLS 回归，然后再考察各模型拟合的情况。该算法分为两个阶段：（1）拟合包含 k 个预测因子的所有模型，其中 k 为模型的最大长度；（2）使用交叉验证预测损失选择单个模型。使用验证或测试误差十分重要，且不能简单地使用训练误差评估模型的拟合情况，这因为 RSS 和 R^2 随变量的增加而单调递增。最好的方法就是通过测试集中最高的 R^2 和最低的 RSS 来交叉验证地选择模型。前向逐步地选择会考虑 p 个预测因子的一个较小子集。它从不含预测因子的模型开始，逐步地添加预测因子到模型中，直到所有预测因子都包含在模型。添加预测因子的顺序是根据不同变量对模型拟合性能提升的程度来确定的，我们会添加变量直到再没有预测因子能在交叉验证误差中提升模型。后向逐步选择先从模型中所有 p 预测器开始，然后迭代地移除用处最小的预测器，每次移除一个。混合法遵循前向逐步方法，但是在添加每个新变量之后，该方法可能还会移除对模型拟合无用的变量。

5. Shrinkage

这种方法涉及到使用所有 p 个预测因子进行建模，然而，估计预测因子重要性的系数将根据最小二乘误差向零收缩。这种收缩也称之为正则化，它旨在减少方差以防止模型的过拟合。由于我们使用不同的收缩方法，有一些变量的估计将归零。因此这种方法也能执行变量的选择，将变量收缩为零最常见的技术就是 Ridge 回归和 Lasso 回归。

Ridge 回归非常类似于最小二乘法，只不过它通过最小化一个稍微不同的数值来估计系数。Ridge 回归和 OLS 一样寻求减少 RSS 的系数估计。然而当系数收缩逼近零值时，它们都会对这种收缩进行惩罚。我们不需要数学分析就能看出 Ridge 回归很擅长于将特征收缩到最小的可能空间中。如主成分分析，Ridge 回归将数据投影到 D 维空间，并在系数空间内收缩较低方差的成分而保留有较高方差的成分。Ridge 回归至少有一个缺点，它需要包含最终模型所有 p 个预测因子，这主要是因为罚项将会令很多预测因子的系数逼近零，但又一定不会等于零。这对于预测准确度来说通常并不是什么问题，但却令模型的结果更难以解释。Lasso 就克服了这一缺点，因为它在 s 组后小的时候能迫使一些预测因子的系数归零。因为 s = 1 将导致正规的 OLS 回归，而当 s 逼近 0 时，系数将收缩到零。因此 Lasso 回归同样是执行变量选择的一个好方法。

6. 降维

降维算法将 p+1 个系数的问题简化为 M+1 个系数的问题，其中 M<p。算法执行包括计算变量的 M 个不同线性组合或投射（projection）。然后这 M 个投射作为预测器通过最小二乘法拟合一个线性回归模型。两个主要的方法是主成分回归（principal component regression）和偏最小二乘法（partial least squares）。

主成分回归（PCR）可以看成一种从大型变量集合中导出低维特征集合的方法。数据中的第一主成分（first principal component）是指观察数据沿着这个变量方向的变化最大。换言之，第一主成分是最接近拟合数据的线，总共可以用 p 个不同的主成分拟合。第二主成分是和第一主成分不相关的变量的线性组合，且在该约束下有最大的方差。其主要思想是主成分能在各个互相垂直的方向使用数据的线性组合捕捉到最大的方差。

使用这种方法，我们还能结合相关变量的效应从数据中获取更多的信息，毕竟在常规的最小二乘法中需要舍弃其中一个相关变量。上面描述的 PCR 方法需要提取 X 的线性组合，以获得对的预测器的最优表征。由于 X 的输出 Y 不能不能用于帮助决定主成分方向，这些组合（方向）使用无监督方法提取。

即，Y 不能监督主成分的提取，从而无法保证这些方向是预测器的最优表征，也无法保证能获得最优预测输出（虽然通常假定如此）。偏最小二乘法（PLS）是一种监督方法，作为 PCR 的代替方法。和 PCR 类似，PLS 也是一种降维方法，它首先提取一个新的较小的特征集合（原始特征的线性组合），然后通过最小二乘法将原来的模型拟合为一个新的具有 M 个特征的线性模型。

7. 非线性模型

在统计学中，非线性回归属于一种回归分析形式，其中，观测数据使用模型参数的非线性组合的函数（依赖于一个或多个独立变量）建模。其使用逐次逼近法拟合数据。下方是几种处理非线性模型的重要技术。

阶梯函数（step function），变量为实数，可以写成区间的指示函数的有限线性组合的形式。非正式的解释是，阶梯函数是一种分段常数函数，只有有限的部分。

分段函数（piecewise function）通过多个子函数定义，每一个子函数被定义在主函数定义域的确定的区间上。分段实际上是一种表示函数的方式，而不是函数自身的特征，但通过额外的限定条件，它可以用于描述函数的本质。

例如，一个分段多项式函数是一个在每一个子定义上为多项式的函数，其中每一个多项式都可能是不同的。样条曲线（spline）是一种用多项式分段定义的特殊函数。

在计算机图形学中，样条曲线是一种分段多项式参数化曲线。由于结构的简单性、评估的简易和高精度、通过曲线拟合和交互曲线设计以逼近复杂曲线的能力，样条曲线很常用。

广义加性模型（generalized additive model）是一种广义线性模型，其中线性预测器线性依赖于某些预测器变量的未知平滑函数，其主要作用就是推测这些平滑函数。

8. 基于树的方法

基于树的方法可以用于回归和分类问题，包括将预测器空间分层或分割成几个简单区域。由于用于预测器空间的分离规则集合可以总结为一个树，这类方法被称为决策树方法。以下的方法是几种不同的树，它们可以组合起来输出单个一致的预测。bagging 能减少预测的方差，即通过从原始数据中生成额外的数据（通过组合和重复生成和原始数据大小相同的多段数据）用于训练。通过增大训练集无法提高模型的预测能力，只能减小方差，仔细地调整预测以得到期望的输出。boosting 是一种计算输出的方法，即使用多个不同的模型，然后使用加权平均的方法对结果取平均值。我们一般通过改变这些方法所占的权重而结合各方法的优势，此外，我们还可以使用不同的精细调整获得对更宽泛输入数据的预测能力。

随机森林算法（random forest algorithm）实际上和 bagging 算法很相似，同样是对训练集提取随机 bootstrap 样本。然而，除了 bootstrap 样本以外，还可以提取特征的随机子集以训练单个树；而在 bagging 中，需要给每个树提供整个特征集。由于特征选择是随机的，相比常规的 bagging 算法，每个树之间更加独立，从而通常能获得更好的预测性能（得益于更好的方差—偏差权衡）。且计算速度也更快，因为每个树只需要学习特征的一个子集。

9. 支持向量机

支持向量机（SVM）是一种常用的监督学习分类技术。通俗地说，它用于寻找对两类点集做出最佳分离的超平面（hyperplane，在 2D 空间中是线，在 3D 空间中是面，在高维空间中是超平面。更正式的说法是，一个超平面是一个 n 维空间的 n-1 维子空间）。而支持向量机是保留最大的间隔的分离超平面，因此本质上，它是一个约束最优化问题，其中支持向量机的间隔在约束下被最大化，从而完美地对数据进行分类（硬间隔分类器）。

那些「支持」着超平面的数据点被称为「支持向量」。在上图中，填充蓝色圆和两个填充方块就是支持向量。在两类数据不是线性可分的例子中，数据点将被投射到一个更高维空间中，使得数据变得线性可分。包含多个类别的数据点的问题可以分解成多个「一对一」（one-versus-one）或「一对剩余」（one-versus-rest）的二分类问题。

10. 无监督学习

目前为止，我们都只讨论过监督学习技术，其中数据分类都是已知的，且提供给算法的经验都是实体和其分类的关系。当数据的分类是未知的时候，就需要使用另一种技术了。它们被称为无监督的，因为它们需要自己去发现数据中的模式。聚类（clustring）是无监督学习的一种，其中数据将根据相关性被分为多个群集。下方是几种最常用的无监督学习算法：

主成分分析：通过保留具备最大方差和互相不相关的特征之间的线性连接，而帮助生成数据集的低维表示。该线性降维技术有助于理解无监督学习中的隐变量交互。k 均值聚类：根据数据到集群中心的距离将其分成 k 个不同的集群。层次聚类：通过数据的层级表示而构建不同的集群。

## 2

用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践

原创2017-03-26清凇CreateAMind

https://zhuanlan.zhihu.com/p/25928551

近来在同时做一个应用深度学习解决淘宝商品的类目预测问题的项目，恰好硕士毕业时论文题目便是文本分类问题，趁此机会总结下文本分类领域特别是应用深度学习解决文本分类的相关的思路、做法和部分实践的经验。

业务问题描述：

淘宝商品的一个典型的例子见下图，图中商品的标题是“夏装雪纺条纹短袖t恤女春半袖衣服夏天中长款大码胖mm显瘦上衣夏”。淘宝网后台是通过树形的多层的类目体系管理商品的，覆盖叶子类目数量达上万个，商品量也是10亿量级，我们是任务是根据商品标题预测其所在叶子类目，示例中商品归属的类目为“女装/女士精品>>蕾丝衫/雪纺衫”。很显然，这是一个非常典型的短文本多分类问题。接下来分别会介绍下文本分类传统和深度学习的做法，最后简单梳理下实践的经验。

一、传统文本分类方法

文本分类问题算是自然语言处理领域中一个非常经典的问题了，相关研究最早可以追溯到上世纪50年代，当时是通过专家规则（Pattern）进行分类，甚至在80年代初一度发展到利用知识工程建立专家系统，这样做的好处是短平快的解决top问题，但显然天花板非常低，不仅费时费力，覆盖的范围和准确率都非常有限。

后来伴随着统计学习方法的发展，特别是90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典玩法，这个阶段的主要套路是人工特征工程+浅层分类模型。训练文本分类器过程见下图：

整个文本分类问题就拆分成了特征工程和分类器两部分，玩机器学习的同学对此自然再熟悉不过了

1.1 特征工程

特征工程在机器学习中往往是最耗时耗力的，但却极其的重要。抽象来讲，机器学习问题是把数据转换成信息再提炼到知识的过程，特征是“数据-->信息”的过程，决定了结果的上限，而分类器是“信息-->知识”的过程，则是去逼近这个上限。然而特征工程不同于分类器模型，不具备很强的通用性，往往需要结合对特征任务的理解。

文本分类问题所在的自然语言领域自然也有其特有的特征处理逻辑，传统分本分类任务大部分工作也在此处。文本特征工程分位文本预处理、特征提取、文本表示三个部分，最终目的是把文本转换成计算机可理解的格式，并封装足够用于分类的信息，即很强的特征表达能力。

1）文本预处理

文本预处理过程是在文本中提取关键词表示文本的过程，中文文本处理中主要包括文本分词和去停用词两个阶段。之所以进行分词，是因为很多研究表明特征粒度为词粒度远好于字粒度，其实很好理解，因为大部分分类算法不考虑词序信息，基于字粒度显然损失了过多“n-gram”信息。

具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。传统算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法。近年来随着深度学习的应用，WordEmbedding + Bi-LSTM+CRF方法逐渐成为主流，本文重点在文本分类，就不展开了。而停止词是文本中一些高频的代词连词介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用表中出现的词，本质上属于特征选择的一部分。

经过文本分词和去停止词之后淘宝商品示例标题变成了下图“ / ”分割的一个个关键词的形式：

夏装 / 雪纺 / 条纹 / 短袖 / t恤 / 女 / 春 / 半袖 / 衣服 / 夏天 / 中长款 / 大码 / 胖mm / 显瘦 / 上衣 / 夏

2）文本表示和特征提取

文本表示：

文本表示的目的是把文本预处理后的转换成计算机可理解的方式，是决定文本分类质量最重要的部分。传统做法常用词袋模型（BOW, Bag Of Words）或向量空间模型（Vector Space Model），最大的不足是忽略文本上下文关系，每个词之间彼此独立，并且无法表征语义信息。词袋模型的示例如下：

( 0, 0, 0, 0, .... , 1, ... 0, 0, 0, 0)

一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高纬度、高稀疏性。词袋模型是向量空间模型的基础，因此向量空间模型通过特征项选择降低维度，通过特征权重计算增加稠密性。

特征提取：

向量空间模型的文本表示方法的特征提取对应特征项的选择和特征权重计算两部分。特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有文档频率、互信息、信息增益、χ²统计量等。

特征权重主要是经典的TF-IDF方法及其扩展方法，主要思路是一个词的重要度与在类别内的词频成正比，与所有类别出现的次数成反比。

3）基于语义的文本表示

传统做法在文本表示方面除了向量空间模型，还有基于语义的文本表示方法，比如LDA主题模型、LSI/PLSI概率潜在语义索引等方法，一般认为这些方法得到的文本表示可以认为文档的深层表示，而word embedding文本分布式表示方法则是深度学习方法的重要基础，下文会展现。

1.2 分类器

分类器基本都是统计分类方法了，基本上大部分机器学习方法都在文本分类领域有所应用，比如朴素贝叶斯分类算法（Naïve Bayes）、KNN、SVM、最大熵和神经网络等等，传统分类模型不是本文重点，在这里就不展开了。

二、深度学习文本分类方法

上文介绍了传统的文本分类做法，传统做法主要问题的文本表示是高纬度高稀疏的，特征表达能力很弱，而且神经网络很不擅长对此类数据的处理；此外需要人工进行特征工程，成本很高。而深度学习最初在之所以图像和语音取得巨大成功，一个很重要的原因是图像和语音原始数据是连续和稠密的，有局部相关性，。应用深度学习解决大规模文本分类问题最重要的是解决文本表示，再利用CNN/RNN等网络结构自动获取特征表达能力，去掉繁杂的人工特征工程，端到端的解决问题。接下来会分别介绍：

2.1 文本的分布式表示：词向量（word embedding）

分布式表示（Distributed Representation）其实Hinton 最早在1986年就提出了，基本思想是将每个词表达成 n 维稠密、连续的实数向量，与之相对的one-hot encoding向量空间只有一个维度是1，其余都是0。分布式表示最大的优点是具备非常powerful的特征表达能力，比如 n 维向量每维 k 个值，可以表征 k^n 个概念。事实上，不管是神经网络的隐层，还是多个潜在变量的概率主题模型，都是应用分布式表示。下图是03年Bengio在 A Neural Probabilistic Language Model 的网络结构：

这篇文章提出的神经网络语言模型（NNLM，Neural Probabilistic Language Model）采用的是文本分布式表示，即每个词表示为稠密的实数向量。NNLM模型的目标是构建语言模型：

词的分布式表示即词向量（word embedding）是训练语言模型的一个附加产物，即图中的Matrix C。

尽管Hinton 86年就提出了词的分布式表示，Bengio 03年便提出了NNLM，词向量真正火起来是google Mikolov 13年发表的两篇word2vec的文章 Efficient Estimation of Word Representations in Vector Space 和 Distributed Representations of Words and Phrases and their Compositionality，更重要的是发布了简单好用的word2vec工具包，在语义维度上得到了很好的验证，极大的推进了文本分析的进程。下图是文中提出的CBOW 和 Skip-Gram两个模型的结构，基本类似于NNLM，不同的是模型去掉了非线性隐层，预测目标不同，CBOW是上下文词预测当前词，Skip-Gram则相反。

除此之外，提出了Hierarchical Softmax 和 Negative Sample两个方法，很好的解决了计算有效性，事实上这两个方法都没有严格的理论证明，有些trick之处，非常的实用主义。详细的过程不再阐述了，有兴趣深入理解word2vec的，推荐读读这篇很不错的paper：word2vec Parameter Learning Explained。额外多提一点，实际上word2vec学习的向量和真正语义还有差距，更多学到的是具备相似上下文的词，比如“good”“bad”相似度也很高，反而是文本分类任务输入有监督的语义能够学到更好的语义表示，有机会后续系统分享下。

至此，文本的表示通过词向量的表示方式，把文本数据从高纬度高稀疏的神经网络难处理的方式，变成了类似图像、语音的的连续稠密数据。深度学习算法本身有很强的数据迁移性，很多之前在图像领域很适用的深度学习算法比如CNN等也可以很好的迁移到文本领域了，下一小节具体阐述下文本分类领域深度学习的方法。

2.2 深度学习文本分类模型

词向量解决了文本表示的问题，该部分介绍的文本分类模型则是利用CNN/RNN等深度学习网络及其变体解决自动特征提取（即特征表达）的问题。

1）fastText

fastText 是上文提到的 word2vec 作者 Mikolov 转战 Facebook 后16年7月刚发表的一篇论文 Bag of Tricks for Efficient Text Classification。把 fastText 放在此处并非因为它是文本分类的主流做法，而是它极致简单，模型图见下：

原理是把句子中所有的词向量进行平均（某种意义上可以理解为只有一个avg pooling特殊CNN），然后直接接 softmax 层。其实文章也加入了一些 n-gram 特征的 trick 来捕获局部序列信息。文章倒没太多信息量，算是“水文”吧，带来的思考是文本分类问题是有一些“线性”问题的部分[from项亮]，也就是说不必做过多的非线性转换、特征组合即可捕获很多分类信息，因此有些任务即便简单的模型便可以搞定了。

2）TextCNN

本篇文章的题图选用的就是14年这篇文章提出的TextCNN的结构（见下图）。fastText 中的网络结果是完全没有考虑词序信息的，而它用的 n-gram 特征 trick 恰恰说明了局部序列信息的重要意义。卷积神经网络（CNN Convolutional Neural Network）最初在图像领域取得了巨大成功，CNN原理就不讲了，核心点在于可以捕捉局部相关性，具体到文本分类任务中可以利用CNN来提取句子中类似 n-gram 的关键信息。

TextCNN的详细过程原理图见下：

TextCNN详细过程：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了。然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示了，最后接一层全连接的 softmax 层，输出每个类别的概率。

特征：这里的特征就是词向量，有静态（static）和非静态（non-static）方式。static方式采用比如word2vec预训练的词向量，训练过程不更新词向量，实质上属于迁移学习了，特别是数据量比较小的情况下，采用静态的词向量往往效果不错。non-static则是在训练过程中更新词向量。推荐的方式是 non-static 中的 fine-tunning方式，它是以预训练（pre-train）的word2vec向量初始化词向量，训练过程中调整词向量，能加速收敛，当然如果有充足的训练数据和资源，直接随机初始化词向量效果也是可以的。

通道（Channels）：图像中可以利用 (R, G, B) 作为不同channel，而文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。

一维卷积（conv-1d）：图像是二维数据，经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。一维卷积带来的问题是需要设计通过不同 filter_size 的 filter 获取不同宽度的视野。

Pooling层：利用CNN解决文本分类问题的文章还是很多的，比如这篇 A Convolutional Neural Network for Modelling Sentences 最有意思的输入是在 pooling 改成 (dynamic) k-max pooling ，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。比如在情感分析场景，举个例子：

“ 我觉得这个地方景色还不错，但是人也实在太多了 ”

虽然前半部分体现情感是正向的，全局文本表达的是偏负面的情感，利用 k-max pooling能够很好捕捉这类信息。

3）TextRNN

尽管TextCNN能够在很多任务里面能有不错的表现，但CNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 "n-gram" 信息。

双向LSTM算是在自然语言处理领域非常一个标配网络了，在序列标注/命名体识别/seq2seq模型等很多场景都有应用，下图是Bi-LSTM用于分类问题的网络结构原理示意图，黄色的节点分别是前向和后向RNN的输出，示例中的是利用最后一个词的结果直接接全连接层softmax输出了。

4）TextRNN + Attention

CNN和RNN用在文本分类任务中尽管效果显著，但都有一个不足的地方就是不够直观，可解释性不好，特别是在分析badcase时候感受尤其深刻。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够很直观的给出每个词对结果的贡献，基本成了Seq2Seq模型的标配了。实际上文本分类从某种意义上也可以理解为一种特殊的Seq2Seq，所以考虑把Attention机制引入近来，研究了下学术界果然有类似做法。

Attention机制介绍：

详细介绍Attention恐怕需要一小篇文章的篇幅，感兴趣的可参考14年这篇paper NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE。

Attention的核心point是在翻译每个目标词（或 预测商品标题文本所属类别）所用的上下文是不同的，这样的考虑显然是更合理的。

TextRNN + Attention 模型：

我们参考了这篇文章 Hierarchical Attention Networks for Document Classification，下图是模型的网络结构图，它一方面用层次化的结构保留了文档的结构，另一方面在word-level和sentence-level。淘宝标题场景只需要 word-level 这一层的 Attention 即可。

加入Attention之后最大的好处自然是能够直观的解释各个句子和词对分类类别的重要性。

5）TextRCNN（TextRNN + CNN）

我们参考的是中科院15年发表在AAAI上的这篇文章 Recurrent Convolutional Neural Networks for Text Classification 的结构：

利用前向和后向RNN得到每个词的前向和后向上下文的表示：

这样词的表示就变成词向量和前向后向上下文向量concat起来的形式了，即：

最后再接跟TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层 filter_size = 1就可以了，不再需要更大 filter_size 获得更大视野，这里词的表示也可以只用双向RNN输出。

三、一点经验

理论和实践之间的Gap往往差异巨大，学术paper更关注的是模型架构设计的新颖性等，更重要的是新的思路；而实践最重要的是在落地场景的效果，关注的点和方法都不一样。这部分简单梳理实际做项目过程中的一点经验教训。

模型显然并不是最重要的：不能否认，好的模型设计对拿到好结果的至关重要，也更是学术关注热点。但实际使用中，模型的工作量占的时间其实相对比较少。虽然再第二部分介绍了5种CNN/RNN及其变体的模型，实际中文本分类任务单纯用CNN已经足以取得很不错的结果了，我们的实验测试RCNN对准确率提升大约1%，并不是十分的显著。最佳实践是先用TextCNN模型把整体任务效果调试到最好，再尝试改进模型。

理解你的数据：虽然应用深度学习有一个很大的优势是不再需要繁琐低效的人工特征工程，然而如果你只是把他当做一个黑盒，难免会经常怀疑人生。一定要理解你的数据，记住无论传统方法还是深度学习方法，数据 sense 始终非常重要。要重视 badcase 分析，明白你的数据是否适合，为什么对为什么错。

关注迭代质量 - 记录和分析你的每次实验：迭代速度是决定算法项目成败的关键，学过概率的同学都很容易认同。而算法项目重要的不只是迭代速度，一定要关注迭代质量。如果你没有搭建一个快速实验分析的套路，迭代速度再快也只会替你公司心疼宝贵的计算资源。建议记录每次实验，实验分析至少回答这三个问题：为什么要实验？结论是什么？下一步怎么实验？

超参调节：超参调节是各位调参工程师的日常了，推荐一篇文本分类实践的论文 A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification，里面贴了一些超参的对比实验，如果你刚开始启动文本分析任务，不妨按文章的结果设置超参，怎么最快的得到超参调节其实是一个非常重要的问题，可以读读 萧瑟的这篇文章 深度学习网络调参技巧 - 知乎专栏。

一定要用 dropout：有两种情况可以不用：数据量特别小，或者你用了更好的正则方法，比如bn。实际中我们尝试了不同参数的dropout，最好的还是0.5，所以如果你的计算资源很有限，默认0.5是一个很好的选择。

fine-tuning 是必选的：上文聊到了，如果只是使用word2vec训练的词向量作为特征表示，我赌你一定会损失很大的效果。

未必一定要 softmax loss： 这取决与你的数据，如果你的任务是多个类别间非互斥，可以试试着训练多个二分类器，我们调整后准确率还是增加了>1%。

类目不均衡问题：基本是一个在很多场景都验证过的结论：如果你的loss被一部分类别dominate，对总体而言大多是负向的。建议可以尝试类似 booststrap 方法调整 loss 中样本权重方式解决。

避免训练震荡：默认一定要增加随机采样因素尽可能使得数据分布iid，默认shuffle机制能使得训练结果更稳定。如果训练模型仍然很震荡，可以考虑调整学习率或 mini_batch_size。

没有收敛前不要过早的下结论：玩到最后的才是玩的最好的，特别是一些新的角度的测试，不要轻易否定，至少要等到收敛吧。

四、写在最后

几年前校招面阿里时，一面二面聊的都是一个文本分类的项目（一个新浪微博主题分类的学校课题项目），用的还是文中介绍的传统的做法。面试时对特征项处理和各个分类器可谓如数家珍，被要求在白板上写了好几个特征选择公式，短短几年传统做法已经被远远超越，不得不感慨深度学习的发展。

值得感慨的一方面是今天技术的发展非常快，故步自封自然是万万万万不可取，深知还有很多理论尚且不懂还要继续深读paper；另一方面，理解理论原理和做好项目间实际非常有巨大的gap，特别是身处工业界的同仁们，学术圈值得钻但要把握分寸，如果仅仅追逐技术深度，不免容易陷入空中阁楼。

最后老规矩再次安利下我们team的招聘，对淘宝搜索排序和自然语言处理方向感兴趣的同学欢迎邮件我 qingsong.huaqs@taobao.com，来淘宝，一起成长！

以上，感谢阅读。

