# 2016-05-06

## 1

神经网络从被人忽悠到忽悠人

神经网络从被人忽悠到忽悠人(一)

原创 2016-04-29 林清漂 

为何取这标题呢，只是觉得目前人工智能只用于娱乐而已。

很早的时候就想写几篇关于人工智能的东西，把人工智能的东西写的通俗易懂点，但是毕竟人工智能的东西涉及的领域太广了，特别是对数学和概率有比较深的理解，如果只是想简单的了解，可以跳过文章的公式。

很难想象有什么事物会像廉价、强大、无处不在的人工智能那样拥有“改变一切”的力量。《必然》

前段时间的AlphaGo再次的把人工智能炒的火热，关于人工智能的讨论又再次进入讨论的风口浪尖上。各个方面对AlphaGo技术的猜测，神经网络也再次成为了技术的焦点。

一个看似简单的问题

给你一堆的图片，从图片中分出是猫，狗。归结成一个大问题：分类。本身来说，分类对计算机来说本该是最擅长的，本身0和1，就是很好的分类，编程语言的if else，swich，可以做到很好的分类。

像if else这种做法，似乎我们可以编写一套复杂的规则，这个规则覆盖所有的情况，就能够进行准确的分类了。但是这条路是走不通的，之前的自然语言处理就走过这条路。需要另外的选择一条出路。对，建模。通过模型来进行分类。

要让机器像人一样的思考，最好的办法就是让他的模型尽量的一样。莱特兄弟发明飞机时，并没有像之前一样利用翅膀，而是通过动力学原理。思考的机器如果靠的是规则下的专家系统，太过复杂，没有规律。最好的一条路是通过数学公式进行建模。

感知器

Hebb在1949年出版的《行为的组织》中，Hebb提出了其神经心理学理论。Hebb认为神经网络的学习过程最终是发生在神经元之间的突触部位，突触的联结强度随着突触前后神经元的活动而变化，变化的量与两个神经元的活性之和成正比。之后人们相继提出了各种各样的学习算法。康奈尔航空实验室心理学家Frank Rosenblatt 受到这种思想的启发，认为这个简单想法足以创造一个可以学会识别物体的机器，在1956年，创建了算法和硬件。1958年，Frank Rosenblatt在《 New York Times 》上发表文章《Electronic ‘Brain’ Teaches Itself.》，正式把算法取名为“感知器”。

1957 年， Frank Rosenblatt 发布了算法模型：

1958年夏，Frank Rosenblatt受到美国海军的经费自助，并召开新闻发布会。《纽约时报》抓住了发布会的要点：

「海军透露了一种电子计算机的雏形，它将能够走路、说话、看、写、自我复制并感知到自己的存在……据预测，不久以后，感知器将能够识别出人并叫出他们的名字，立即把演讲内容翻译成另一种语言并写下来。」

现在来看这段话，就能看出Frank Rosenblatt靠谱的预见性了。可是这些事情在当时看来远超人们的想象，认为Frank Rosenblatt天方夜谭，像小孩子一样想象着未来。

感知器是有单层计算单元的神经网络，由线性元件及阀值元件组成。感知器如图所示。再来看 神经网络的学习过程最终是发生在神经元之间的突触部位，突触的联结强度随着突触前后神经元的活动而变化，变化的量与两个神经元的活性之和成正比。 Frank Rosenblatt用数学的方法描述这个过程。

感知器的数学模型：

其中： f[.] 是 阶跃函数 ，并且有

θ是阀值。感知器的最大作用就是可以对输入的样本分类，故它可作分类器，感知器对输入信号的分类如下（A类，B类）：

当感知器的输出为1时，输入样本称为A类；输出为-1时，输入样本称为B类。从上可知感知器的分类边界是：

在输入样本只有两个分量X1，X2时，则有分类边界条件：

即

也可写成

这时的分类情况如图所示。

感知器算法

感知器的学习算法： 目的在于找寻恰当的权系数(W1…Wn)，使系统对一个特 定的样本(X1…Xn)能产生期望值d。

感知器学习算法步骤如下：对权系数置初值。输入一样(X1…Xn)本以及它的期望输出d。期望输出值d在样本的类属不同时取值不同。如果是A类，则取d＝1,如果是B类，则取-1。期望输出d也即是教师信号。计算实际输出值。

根据实际输出求误差e。

用误差e去修改权系数。

转到第2点，一直执行到一切样本均稳定为止。

感知器是整个神经网络的基础，神经元通过激励函数确定输出，神经元之间通过权值进行传递能量，权重的确定根据误差来进行调节（这个就是学习的过程），这个方法的前提是整个网络是收敛的。这个问题，1957年Frank Rosenblatt证明了这个结论。

或许这才是开始

但是1969年，Minsky 和Papert所著的《Perceptron》一书出版，该书从数学角度证明了关于单层感知器的计算具有根本的局限性，指出感知器的处理能力有限，甚至连XOR这样的问题也不能解决，并在多层感知器的总结中，论述了单层感知器的所有局限性在多层感知器中是不可能被全部克服的。神经网络进入了萧条期。

Marvin Minsky是“人工智能之父”，1970年，Minsky获得了计算机科学界最高奖项——图灵奖（the Turing Award），同时他也是第一位获此殊荣的人工智能学者。2016 年1月24日，上帝着需要人工智能，带走了Marvin Minsky，享年89岁。

这才是开始，2004年IEEE Frank Rosenblatt Award成立，Frank Rosenblatt被尊称为神经网络的创立者。

神经网络开启了人类对大脑的模拟形式，一种新型对大脑的建模，在这条路上后续有更多的科学家前仆后继，我们是树下乘凉的人。

神经网络从被人忽悠到忽悠人（二）

原创 2016-05-05 林清漂 

引子

Minsky说过神经网络无法解决异或问题。60年代关于神经网络的研究还取得了一定的进展，但是都没有取得重大的突破。70年代，神经网络的研究进入了萧条期，人工智能里产生了许多不同的方向，神经网络，好像被人们所忘记。

直到1986年，David Rumelhar和Geoffery Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，克服了Minsky说过神经网络无法解决异或问题，该算法是对神经网络受到批判的一个有力的回答。

其中Geoffery Hinton是神经网络的发展中一个重要的人物，对神经网络坚持，使得他获得了很高的成就，2013年加入google，成为google神经网络的领军人物。

Geoffery Hinton

BP神经网络

从感如器的学习算法可知，学习的目的是在于修改网络中的权系数，使到网络对于所输入的模式样本能正确分类。当学习结束时，也即神经网络能正确分类时，显然 权系数就反映了同类输人模式样本的共同特征。换句话讲，权系数就是存储了的输人模式。由于权系数是分散存在的，故神经网络自然而然就有分布存储的特点。

能够求出网络中的权系数，是神经网络的关键。在bp神经网络中，采用的是误差回传，从结果的误差，回传到中间层，利用梯度算法，计算权重，为了能实现梯度算法，故把神经元的激发函数改为可微分函数，例如Sigmoid函数：

最后又按负梯度方向修改权系数W的修改规则：

mu 是权重变化率，它视情况不同而取值不同，一般取0-1之间的小数。很明显，梯度法比原来感知器的学习算法进了一大步。其关键在于两点：

1. 神经元的传递函数采用连续的s型函数，而不是阶跃函数；

2. 对权系数的修改采用误差的梯度去控制，而不是采用误差去控制。故而有更好的动态特能，即加强了收敛进程。

bp神经网络算法的推导过程有点复杂，最后的结果为：

算法的执行的步骤如下：

1. 对权系数Wij置初值。对各层的权系数置一个较小的非零随机数。

2. 输入一个样本，以及对应期望输出。

3. 计算各层的输出，对于第k层第i个神经元的输出，有：

4．求各层的学习误差,对于输出层有k＝m，有

对于其他各层，有

5．修正权系数Wij和阀值θ

6．当求出了各层各个权系数之后，可按给定品质指标判别是否满足要求。如果满足要求，则算法结束；如果未满足要求，则返回(3)执行。 这个学习过程，对于任一给定的样本和期望输出都要执行，直到满足所有输入输出要求为止。

许多问题

历史总是惊人的相似，神经网络的学者们再次登上了《纽约时报》的专访。人们认为神经网络可以解决许多问题。就连娱乐界都开始受到了影响，当年的《终结者》电影中的阿诺都赶时髦地说一句：我的CPU是一个神经网络处理器，一个会学习的计算机。

但是神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。

又一次寒冬

Yann Lecun 1960年 出生于巴黎。1987年 在法国获得博士学位后，他曾追随 Hinton 教授到多伦多大学做了一年博士后的工作，随后搬到新泽西州的贝尔实验室继续研究工作。

在贝尔实验室，Yann Lecun 1989年 发表了论文， “反向传播算法在手写邮政编码上的应用”。他用美国邮政系统提供的近万个手写数字的样本来培训神经网络系统，培训好的系统在独立的测试样本中， 错误率只有 5%。

Yann Lecun 进一步运用一种叫做“卷积神经网络” （Convoluted Neural Networks） 的技术，开发出商业软件用于读取银行支票上的手写数字，这个支票识别系统在九十年代末占据了美国接近 20%的市场。

至今在 Yann Lecun的网站里还能看到Convoluted Neural Networks的算法，简称为LeNet 5，下图是算法的动态演示图，能够清晰的看到手写数字被正确的识别为相应的数字，对代码有兴趣的同学，github上有很多相关的LeNet 5代码。

然而神经网络还是存在着上面所述的缺陷，此时就在贝尔实验室，Yann Lecun临近办公室的一个同事的工作，又把神经网络的研究带入第二个寒冬。

## 2

关于深度学习看这一篇就够了

编者按：本文作者王川，投资人，中科大少年班校友，现居加州硅谷，个人微信号9935070，36 氪经授权转载自其个人微信公众号 investguru。

一

2016 年一月底，人工智能的研究领域，发生了两件大事。

先是一月二十四号，MIT 的教授，人工智能研究的先驱者，Marvin Minsky 去世，享年89 岁。

三天之后，谷歌在自然杂志上正式公开发表论文，宣布其以深度学习技术为基础的电脑程序 AlphaGo， 在 2015年 十月，连续五局击败欧洲冠军、职业二段樊辉。

这是第一次机器击败职业围棋选手。距离 97年IBM 电脑击败国际象棋世界冠军，一晃近二十年了。

极具讽刺意义的是，Minsky 教授，一直不看好深度学习的概念。他曾在 1969年 出版了 Perceptron （感知器） 一书，指出了神经网络技术 （就是深度学习的前身） 的局限性。这本书直接导致了神经网络研究的将近二十年的长期低潮。

神经网络研究的历史，是怎样的？

深度学习有多深？学了究竟有几分？

二

人工智能研究的方向之一，是以所谓 “专家系统” 为代表的，用大量 “如果-就” （If - Then） 规则定义的，自上而下的思路。

人工神经网络 （ Artifical Neural Network），标志着另外一种自下而上的思路。

神经网络没有一个严格的正式定义。它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。

一个计算模型，要划分为神经网络，通常需要大量彼此连接的节点 （也称 ‘神经元’），并且具备两个特性：

每个神经元，通过某种特定的输出函数 （也叫激励函数 activation function），计算处理来自其它相邻神经元的加权输入值

神经元之间的信息传递的强度，用所谓加权值来定义，算法会不断自我学习，调整这个加权值

在此基础上，神经网络的计算模型，依靠大量的数据来训练，还需要：

成本函数 （cost function）：用来定量评估根据特定输入值， 计算出来的输出结果，离正确值有多远，结果有多靠谱

学习的算法 （ learning algorithm ）：这是根据成本函数的结果， 自学， 纠错， 最快地找到神经元之间最优化的加权值

用小明、小红和隔壁老王们都可以听懂的语言来解释，神经网络算法的核心就是：计算、连接、评估、纠错、疯狂培训。

随着神经网络研究的不断变迁，其计算特点和传统的生物神经元的连接模型渐渐脱钩。

但是它保留的精髓是：非线性、分布式、并行计算、自适应、自组织。

三

神经网络作为一个计算模型的理论，1943年 最初由科学家 Warren McCulloch 和 Walter Pitts 提出。

康内尔大学教授 Frank Rosenblatt 1957年 提出的“感知器” （Perceptron），是第一个用算法来精确定义神经网络，第一个具有自组织自学习能力的数学模型，是日后许多新的神经网络模型的始祖。

Rosenblatt 乐观地预测，感知器最终可以 “学习、做决定、翻译语言”。感知器的技术，六十年代一度走红，美国海军曾出资支持这个技术的研究，期望它 “以后可以自己走、说话、看、读、自我复制、甚至拥有自我意识”。

Rosenblatt 和 Minsky 实际上是间隔一级的高中校友。但是六十年代，两个人在感知器的问题上展开了长时间的激辩。Rosenblatt 认为感应器将无所不能，Minsky 则认为它应用有限。

1969 年，Marvin Minsky 和 Seymour Papert 出版了新书：“感知器：计算几何简介”。书中论证了感知器模型的两个关键问题：

第一，单层的神经网络无法解决不可线性分割的问题，典型例子如异或门，XOR Circuit （ 通俗地说，异或门就是：两个输入如果是异性恋，输出为一。两个输入如果是同性恋，输出为零 ）

第二，更致命的问题是，当时的电脑完全没有能力完成神经网络模型所需要的超大的计算量。

此后的十几年，以神经网络为基础的人工智能研究进入低潮，相关项目长期无法得到政府经费支持，这段时间被称为业界的核冬天。

Rosenblatt 自己则没有见证日后神经网络研究的复兴。1971年 他 43 岁生日时，不幸在海上开船时因为事故而丧生。

1970年，当神经网络研究的第一个寒冬降临时。在英国的爱丁堡大学，一位二十三岁的年轻人，Geoffrey Hinton，刚刚获得心理学的学士学位。

Hinton 六十年代还是中学生时，就对脑科学着迷。当时一个同学给他介绍关于大脑记忆的理论是：

大脑对于事物和概念的记忆，不是存储在某个单一的地点，而是像全息照片一样，分布式地，存在于一个巨大的神经元的网络里。

分布式表征 （Distributed Representation），是神经网络研究的一个核心思想。

它的意思是，当你表达一个概念的时候，不是用单个神经元一对一地存储定义；概念和神经元是多对多的关系：一个概念可以用多个神经元共同定义表达，同时一个神经元也可以参与多个不同概念的表达。

举个最简单的例子。一辆 “大白卡车”，如果分布式地表达，一个神经元代表大小，一个神经元代表颜色，第三个神经元代表车的类别。三个神经元同时激活时，就可以准确描述我们要表达的物体。

分布式表征和传统的局部表征 （localized representation） 相比，存储效率高很多。线性增加的神经元数目，可以表达指数级增加的大量不同概念。

分布式表征的另一个优点是，即使局部出现硬件故障，信息的表达不会受到根本性的破坏。

这个理念让 Hinton 顿悟，使他四十多年来，一直在神经网络研究的领域里坚持下来没有退缩。

本科毕业后，Hinton 选择继续在爱丁堡大学读研，把人工智能作为自己的博士研究方向。

周围的一些朋友对此颇为不解。“你疯了吗？ 为什么浪费时间在这些东西上？ 这 （神经网络） 早就被证明是扯淡的东西了。”

Hinton 1978 年在爱丁堡获得博士学位后，来到美国继续他的研究工作。

二

神经网络当年被 Minsky 诟病的问题之一是巨大的计算量。

简单说，传统的感知器用所谓 “梯度下降”的算法纠错时，耗费的计算量和神经元数目的平方成正比。当神经元数目增多，庞大的计算量是当时的硬件无法胜任的。

1986年 七月，Hinton 和 David Rumelhart 合作在自然杂志上发表论文， “Learning Representations by Back-propagating errors”，第一次系统简洁地阐述反向传播算法在神经网络模型上的应用。

反向传播算法，把纠错的运算量下降到只和神经元数目本身成正比。

反向传播算法，通过在神经网络里增加一个所谓隐层 （hidden layer），同时也解决了感知器无法解决异或门 （XOR gate） 的难题。

使用了反向传播算法的神经网络，在做诸如形状识别之类的简单工作时，效率比感知器大大提高。

八十年代末计算机的运行速度，也比二十年前高了几个数量级。

神经网络的研究开始复苏。

三

Yann Lecun （我给他取个中文名叫 “严乐春”吧） 1960年 出生于巴黎。1987年 在法国获得博士学位后，他曾追随 Hinton 教授到多伦多大学做了一年博士后的工作，随后搬到新泽西州的贝尔实验室继续研究工作。

在贝尔实验室，严乐春 1989年 发表了论文， “反向传播算法在手写邮政编码上的应用”。他用美国邮政系统提供的近万个手写数字的样本来培训神经网络系统，培训好的系统在独立的测试样本中， 错误率只有 5%。

严乐春进一步运用一种叫做“卷积神经网络” （Convoluted Neural Networks） 的技术，开发出商业软件用于读取银行支票上的手写数字，，这个支票识别系统在九十年代末占据了美国接近 20%的市场。

此时就在贝尔实验室，严乐春临近办公室的一个同事的工作，又把神经网络的研究带入第二个寒冬。

四

Vladmir Vapnik，1936年 出生于前苏联，90年 移民到美国，在贝尔实验室做研究。

早在 1963年，Vapnik 就提出了 支持向量机 （Support Vector Machine） 的算法。支持向量机，是一种精巧的分类算法。

除了基本的线性分类外，在数据样本线性不可分的时候，SVM 使用所谓 “核机制” （kernel trick） 的非线性映射算法，将线性不可分的样本转化到高维特征空间 （high-dimensional feature space），使其线性可分。

SVM，作为一种分类算法，九十年代初开始，在图像和语音识别上找到了广泛的用途。

在贝尔实验室的走廊上，严乐春和 Vapnik 常常就 （深度） 神经网络和 SVM 两种技术的优缺点，展开热烈的讨论。

Vapnik 的观点是：SVM，非常精巧地在 “容量调节” （Capacity Control） 上 选择一个合适的平衡点，而这是神经网络不擅长的。

什么是 “容量调节”？ 举个简单的例子：如果算法容量太大，就像一个记忆力极为精准的植物学家，当她看到一颗新的树的时候，由于这棵树的叶子和她以前看到的树的叶子数目不一样，所以她判断这不是树；如果算法容量太小，就像一个懒惰的植物学家，只要看到绿色的东西都把它叫做树。

严乐春的观点是：用有限的计算能力，解决高度复杂的问题，比“容量调节”更重要。支持向量机，虽然算法精巧，但本质就是一个双层神经网络系统。它的最大的局限性，在于其“核机制”的选择。当图像识别技术需要忽略一些噪音信号时，卷积神经网络的技术，计算效率就比 SVM 高的多。

在手写邮政编码的识别问题上，SVM 的技术不断进步，1998年 就把错误率降到低于 0.8%，2002年 最低达到了 0.56%，这远远超越同期传统神经网络算法的表现。

神经网络的计算，在实践中还有另外两个主要问题：

第一，算法经常停止于局部最优解，而不是全球最优解。这好比“只见树木，不见森林”。

第二，算法的培训，时间过长时，会出现过度拟合 （overfit），把噪音当做有效信号。

